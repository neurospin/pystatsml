{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn processing pipelines\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "Sources: http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html\n",
    "\n",
    "### Encoding categorical features\n",
    "\n",
    "To be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization of input features\n",
    "\n",
    "Sources:\n",
    "\n",
    "- http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "- http://stats.stackexchange.com/questions/111017/question-about-standardizing-in-ridge-regression\n",
    "\n",
    "\n",
    "\"Standardizing\" or mean removal and variance scaling, is not systematic. For example multiple linear regression does not require it. However it is a good practive in many cases:\n",
    "\n",
    "- The **variable combination method is sensitive scales**. If the input variables are combined via a distance function (such as Euclidean distance) in an RBF network, standardizing inputs can be crucial. The contribution of an input will depend heavily on its variability relative to other inputs. If one input has a range of 0 to 1, while another input has a range of 0 to 1,000,000, then the contribution of the first input to the distance will be swamped by the second input.\n",
    "\n",
    "- **Regularized learning algorithm**. Lasso or Ridge regression regularize the linear regression by imposing a penalty on the size of coefficients. Thus the coefficients are shrunk toward zero and toward each other. But when this happens and if the independent variables does not have the same scale, the shrinking is not fair. Two independent variables with different scales will have different contributions to the penalized terms, because the penalized term is norm (a sum of squares, or absolute values) of all the coefficients. To avoid such kind of problems, very often, the independent variables are centered and scaled in order to have variance 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Linear regression: scaling is not required ==\n",
      "Coefficients: [  1.05421281e-04   1.13551103e+02   9.78705905e+01   1.60747221e+01\n",
      "  -7.23145329e-01] -113550117.827\n",
      "Test R2:0.77\n",
      "== Lasso without scaling ==\n",
      "Coefficients: [  8.61125764e-05   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00] 986.15608907\n",
      "Test R2:0.09\n",
      "== Lasso with scaling ==\n",
      "Coefficients: [  87.46834069  105.13635448   91.22718731    9.22953206   -0.        ] 982.302793647\n",
      "Test R2:0.77\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# dataset\n",
    "np.random.seed(42)\n",
    "n_samples, n_features, n_features_info = 100, 5, 3\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "beta = np.zeros(n_features)\n",
    "beta[:n_features_info] = 1\n",
    "Xbeta = np.dot(X, beta)\n",
    "eps = np.random.randn(n_samples)\n",
    "y = Xbeta + eps\n",
    "\n",
    "X[:, 0] *= 1e6  # inflate the first feature\n",
    "X[:, 1] += 1e6  # bias the second feature\n",
    "y = 100 * y + 1000  # bias and scale the output\n",
    "\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "print(\"== Linear regression: scaling is not required ==\")\n",
    "model =lm.LinearRegression()\n",
    "model.fit(X, y)\n",
    "print(\"Coefficients:\", model.coef_, model.intercept_)\n",
    "print(\"Test R2:%.2f\" % cross_val_score(estimator=model, X=X, y=y, cv=5).mean())\n",
    "\n",
    "print(\"== Lasso without scaling ==\")\n",
    "model = lm.LassoCV()\n",
    "model.fit(X, y)\n",
    "print(\"Coefficients:\", model.coef_, model.intercept_)\n",
    "print(\"Test R2:%.2f\" % cross_val_score(estimator=model, X=X, y=y, cv=5).mean())\n",
    "\n",
    "print(\"== Lasso with scaling ==\")\n",
    "model = lm.LassoCV()\n",
    "scaler = preprocessing.StandardScaler()\n",
    "Xc = scaler.fit(X).transform(X)\n",
    "model.fit(Xc, y)\n",
    "print(\"Coefficients:\", model.coef_, model.intercept_)\n",
    "print(\"Test R2:%.2f\" % cross_val_score(estimator=model, X=Xc, y=y, cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn pipelines\n",
    "\n",
    "Sources: http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "Note that statistics such as the mean and standard deviation are computed from the training data, not from the validation or test data. The validation and test data must be standardized using the statistics computed from the training data. Thus Standardization should be merged together with the learner using a ``Pipeline``.\n",
    "\n",
    "Pipeline chain multiple estimators into one. All estimators in a pipeline, except the last one, must have the ``fit()`` and ``transform()`` methods. The last must implement the ``fit()`` and ``predict()`` methods.\n",
    "\n",
    "### Standardization of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  r2:0.77\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import sklearn.linear_model as lm\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "model = make_pipeline(preprocessing.StandardScaler(), lm.LassoCV())\n",
    "\n",
    "# or\n",
    "from sklearn.pipeline import Pipeline\n",
    "model = Pipeline([('standardscaler', preprocessing.StandardScaler()), \n",
    "                  ('lassocv', lm.LassoCV())])\n",
    "\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=5)\n",
    "print(\"Test  r2:%.2f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features selection\n",
    "\n",
    "An alternative to features selection based on $\\ell_1$ penalty is to use a preprocessing stp of univariate feature selection.\n",
    "\n",
    "Such methods, called **filters**, are a simple, widely used method for supervised dimension reduction [26]. Filters are univariate methods that rank features according to their ability to predict the target, independently of other features. This ranking may be based on parametric (e.g., t-tests) or nonparametric (e.g., Wilcoxon tests) statistical methods. Filters are computationally efficient and more robust to overfitting than multivariate methods. However, they are blind to feature interrelations, a problem that can be addressed only with multivariate selection such as learning with $\\ell_1$ penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anova filter + linear regression, test  r2:0.72\n",
      "Standardize + Lasso, test  r2:0.66\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples, n_features, n_features_info = 100, 100, 3\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "beta = np.zeros(n_features)\n",
    "beta[:n_features_info] = 1\n",
    "Xbeta = np.dot(X, beta)\n",
    "eps = np.random.randn(n_samples)\n",
    "y = Xbeta + eps\n",
    "\n",
    "X[:, 0] *= 1e6  # inflate the first feature\n",
    "X[:, 1] += 1e6  # bias the second feature\n",
    "y = 100 * y + 1000  # bias and scale the output\n",
    "\n",
    "model = Pipeline([('anova', SelectKBest(f_regression, k=3)),\n",
    "                  ('lm', lm.LinearRegression())])\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=5)\n",
    "print(\"Anova filter + linear regression, test  r2:%.2f\" % scores.mean())\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "model = Pipeline([('standardscaler', preprocessing.StandardScaler()),\n",
    "                  ('lassocv', lm.LassoCV())])\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=5)\n",
    "print(\"Standardize + Lasso, test  r2:%.2f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression pipelines with CV for parameters selection\n",
    "\n",
    "Now we combine standardization of input features, feature selection and learner with hyper-parameter within a pipeline which is warped in a grid search procedure to select the best hyperparameters based on a (inner)CV. The overall is plugged in an outer CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR: 3.28668201676\n",
      "=============================\n",
      "== Basic linear regression ==\n",
      "=============================\n",
      "Test  r2:0.29\n",
      "==============================================\n",
      "== Scaler + anova filter + ridge regression ==\n",
      "==============================================\n",
      "----------------------------\n",
      "-- Parallelize inner loop --\n",
      "----------------------------\n",
      "CPU times: user 6.06 s, sys: 836 ms, total: 6.9 s\n",
      "Wall time: 7.97 s\n",
      "Test r2:0.86\n",
      "----------------------------\n",
      "-- Parallelize outer loop --\n",
      "----------------------------\n",
      "CPU times: user 270 ms, sys: 129 ms, total: 399 ms\n",
      "Wall time: 3.51 s\n",
      "Test r2:0.86\n",
      "=====================================\n",
      "== Scaler + Elastic-net regression ==\n",
      "=====================================\n",
      "----------------------------\n",
      "-- Parallelize outer loop --\n",
      "----------------------------\n",
      "CPU times: user 44.4 ms, sys: 80.5 ms, total: 125 ms\n",
      "Wall time: 1.43 s\n",
      "Test r2:0.82\n",
      "-----------------------------------------------\n",
      "-- Parallelize outer loop + built-in CV      --\n",
      "-- Remark: scaler is only done on outer loop --\n",
      "-----------------------------------------------\n",
      "CPU times: user 227 ms, sys: 0 ns, total: 227 ms\n",
      "Wall time: 225 ms\n",
      "Test r2:0.82\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Datasets\n",
    "n_samples, n_features, noise_sd = 100, 100, 20\n",
    "X, y, coef = datasets.make_regression(n_samples=n_samples, n_features=n_features, \n",
    "                                      noise=noise_sd, n_informative=5,\n",
    "                                      random_state=42, coef=True)\n",
    " \n",
    "# Use this to tune the noise parameter such that snr < 5\n",
    "print(\"SNR:\", np.std(np.dot(X, coef)) / noise_sd)\n",
    "\n",
    "print(\"=============================\")\n",
    "print(\"== Basic linear regression ==\")\n",
    "print(\"=============================\")\n",
    "\n",
    "scores = cross_val_score(estimator=lm.LinearRegression(), X=X, y=y, cv=5)\n",
    "print(\"Test  r2:%.2f\" % scores.mean())\n",
    "\n",
    "print(\"==============================================\")\n",
    "print(\"== Scaler + anova filter + ridge regression ==\")\n",
    "print(\"==============================================\")\n",
    "\n",
    "anova_ridge = Pipeline([\n",
    "    ('standardscaler', preprocessing.StandardScaler()),\n",
    "    ('selectkbest', SelectKBest(f_regression)),\n",
    "    ('ridge', lm.Ridge())\n",
    "])\n",
    "param_grid = {'selectkbest__k':np.arange(10, 110, 10), \n",
    "              'ridge__alpha':[.001, .01, .1, 1, 10, 100] }\n",
    "\n",
    "# Expect execution in ipython, for python remove the %time\n",
    "print(\"----------------------------\")\n",
    "print(\"-- Parallelize inner loop --\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "anova_ridge_cv = GridSearchCV(anova_ridge, cv=5,  param_grid=param_grid, n_jobs=-1)\n",
    "%time scores = cross_val_score(estimator=anova_ridge_cv, X=X, y=y, cv=5)\n",
    "print(\"Test r2:%.2f\" % scores.mean())\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"-- Parallelize outer loop --\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "anova_ridge_cv = GridSearchCV(anova_ridge, cv=5,  param_grid=param_grid)\n",
    "%time scores = cross_val_score(estimator=anova_ridge_cv, X=X, y=y, cv=5, n_jobs=-1)\n",
    "print(\"Test r2:%.2f\" % scores.mean())\n",
    "\n",
    "\n",
    "print(\"=====================================\")\n",
    "print(\"== Scaler + Elastic-net regression ==\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "alphas = [.0001, .001, .01, .1, 1, 10, 100, 1000] \n",
    "l1_ratio = [.1, .5, .9]\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"-- Parallelize outer loop --\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "enet = Pipeline([\n",
    "    ('standardscaler', preprocessing.StandardScaler()),\n",
    "    ('enet', lm.ElasticNet(max_iter=10000)),\n",
    "])\n",
    "param_grid = {'enet__alpha':alphas ,\n",
    "              'enet__l1_ratio':l1_ratio}\n",
    "enet_cv = GridSearchCV(enet, cv=5,  param_grid=param_grid)\n",
    "%time scores = cross_val_score(estimator=enet_cv, X=X, y=y, cv=5, n_jobs=-1)\n",
    "print(\"Test r2:%.2f\" % scores.mean())\n",
    "\n",
    "print(\"-----------------------------------------------\")\n",
    "print(\"-- Parallelize outer loop + built-in CV      --\")\n",
    "print(\"-- Remark: scaler is only done on outer loop --\")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "enet_cv = Pipeline([\n",
    "    ('standardscaler', preprocessing.StandardScaler()),\n",
    "    ('enet', lm.ElasticNetCV(max_iter=10000, l1_ratio=l1_ratio, alphas=alphas)),\n",
    "])\n",
    "\n",
    "%time scores = cross_val_score(estimator=enet_cv, X=X, y=y, cv=5)\n",
    "print(\"Test r2:%.2f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification pipelines with CV for parameters selection\n",
    "\n",
    "Now we combine standardization of input features, feature selection and learner with hyper-parameter within a pipeline which is warped in a grid search procedure to select the best hyperparameters based on a (inner)CV. The overall is plugged in an outer CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================\n",
      "== Basic logistic regression ==\n",
      "=============================\n",
      "Test  bACC:0.52\n",
      "=======================================================\n",
      "== Scaler + anova filter + ridge logistic regression ==\n",
      "=======================================================\n",
      "----------------------------\n",
      "-- Parallelize inner loop --\n",
      "----------------------------\n",
      "CPU times: user 3.02 s, sys: 562 ms, total: 3.58 s\n",
      "Wall time: 4.43 s\n",
      "Test bACC:0.67\n",
      "----------------------------\n",
      "-- Parallelize outer loop --\n",
      "----------------------------\n",
      "CPU times: user 59.3 ms, sys: 114 ms, total: 174 ms\n",
      "Wall time: 1.88 s\n",
      "Test bACC:0.67\n",
      "========================================\n",
      "== Scaler + lasso logistic regression ==\n",
      "========================================\n",
      "----------------------------\n",
      "-- Parallelize outer loop --\n",
      "----------------------------\n",
      "CPU times: user 81 ms, sys: 96.7 ms, total: 178 ms\n",
      "Wall time: 484 ms\n",
      "Test bACC:0.57\n",
      "-----------------------------------------------\n",
      "-- Parallelize outer loop + built-in CV      --\n",
      "-- Remark: scaler is only done on outer loop --\n",
      "-----------------------------------------------\n",
      "CPU times: user 575 ms, sys: 3.01 ms, total: 578 ms\n",
      "Wall time: 327 ms\n",
      "Test bACC:0.60\n",
      "=============================================\n",
      "== Scaler + Elasticnet logistic regression ==\n",
      "=============================================\n",
      "----------------------------\n",
      "-- Parallelize outer loop --\n",
      "----------------------------\n",
      "CPU times: user 429 ms, sys: 100 ms, total: 530 ms\n",
      "Wall time: 979 ms\n",
      "Test bACC:0.61\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Datasets\n",
    "n_samples, n_features, noise_sd = 100, 100, 20\n",
    "X, y = datasets.make_classification(n_samples=n_samples, n_features=n_features,\n",
    "                         n_informative=5, random_state=42)\n",
    "\n",
    "\n",
    "def balanced_acc(estimator, X, y):\n",
    "    '''\n",
    "    Balanced acuracy scorer\n",
    "    '''\n",
    "    return metrics.recall_score(y, estimator.predict(X), average=None).mean()\n",
    "\n",
    "print(\"=============================\")\n",
    "print(\"== Basic logistic regression ==\")\n",
    "print(\"=============================\")\n",
    "\n",
    "scores = cross_val_score(estimator=lm.LogisticRegression(C=1e8, class_weight='balanced'),\n",
    "                         X=X, y=y, cv=5, scoring=balanced_acc)\n",
    "print(\"Test  bACC:%.2f\" % scores.mean())\n",
    "\n",
    "print(\"=======================================================\")\n",
    "print(\"== Scaler + anova filter + ridge logistic regression ==\")\n",
    "print(\"=======================================================\")\n",
    "\n",
    "anova_ridge = Pipeline([\n",
    "    ('standardscaler', preprocessing.StandardScaler()),\n",
    "    ('selectkbest', SelectKBest(f_classif)),\n",
    "    ('ridge', lm.LogisticRegression(penalty='l2', class_weight='balanced'))\n",
    "])\n",
    "param_grid = {'selectkbest__k':np.arange(10, 110, 10), \n",
    "              'ridge__C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "\n",
    "# Expect execution in ipython, for python remove the %time\n",
    "print(\"----------------------------\")\n",
    "print(\"-- Parallelize inner loop --\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "anova_ridge_cv = GridSearchCV(anova_ridge, cv=5,  param_grid=param_grid, \n",
    "                              scoring=balanced_acc, n_jobs=-1)\n",
    "%time scores = cross_val_score(estimator=anova_ridge_cv, X=X, y=y, cv=5,\\\n",
    "                               scoring=balanced_acc)\n",
    "print(\"Test bACC:%.2f\" % scores.mean())\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"-- Parallelize outer loop --\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "anova_ridge_cv = GridSearchCV(anova_ridge, cv=5,  param_grid=param_grid,\n",
    "                              scoring=balanced_acc)\n",
    "%time scores = cross_val_score(estimator=anova_ridge_cv, X=X, y=y, cv=5,\\\n",
    "                               scoring=balanced_acc, n_jobs=-1)\n",
    "print(\"Test bACC:%.2f\" % scores.mean())\n",
    "\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"== Scaler + lasso logistic regression ==\")\n",
    "print(\"========================================\")\n",
    "\n",
    "Cs = np.array([.0001, .001, .01, .1, 1, 10, 100, 1000, 10000])\n",
    "alphas = 1 / Cs\n",
    "l1_ratio = [.1, .5, .9]\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"-- Parallelize outer loop --\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "lasso = Pipeline([\n",
    "    ('standardscaler', preprocessing.StandardScaler()),\n",
    "    ('lasso', lm.LogisticRegression(penalty='l1', class_weight='balanced')),\n",
    "])\n",
    "param_grid = {'lasso__C':Cs}\n",
    "enet_cv = GridSearchCV(lasso, cv=5,  param_grid=param_grid, scoring=balanced_acc)\n",
    "%time scores = cross_val_score(estimator=enet_cv, X=X, y=y, cv=5,\\\n",
    "                               scoring=balanced_acc, n_jobs=-1)\n",
    "print(\"Test bACC:%.2f\" % scores.mean())\n",
    "\n",
    "\n",
    "print(\"-----------------------------------------------\")\n",
    "print(\"-- Parallelize outer loop + built-in CV      --\")\n",
    "print(\"-- Remark: scaler is only done on outer loop --\")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "lasso_cv = Pipeline([\n",
    "    ('standardscaler', preprocessing.StandardScaler()),\n",
    "    ('lasso', lm.LogisticRegressionCV(Cs=Cs, scoring=balanced_acc)),\n",
    "])\n",
    "\n",
    "%time scores = cross_val_score(estimator=lasso_cv, X=X, y=y, cv=5)\n",
    "print(\"Test bACC:%.2f\" % scores.mean())\n",
    "\n",
    "\n",
    "print(\"=============================================\")\n",
    "print(\"== Scaler + Elasticnet logistic regression ==\")\n",
    "print(\"=============================================\")\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"-- Parallelize outer loop --\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "enet = Pipeline([\n",
    "    ('standardscaler', preprocessing.StandardScaler()),\n",
    "    ('enet', lm.SGDClassifier(loss=\"log\", penalty=\"elasticnet\",\n",
    "                            alpha=0.0001, l1_ratio=0.15, class_weight='balanced')),\n",
    "])\n",
    "\n",
    "param_grid = {'enet__alpha':alphas,\n",
    "              'enet__l1_ratio':l1_ratio}\n",
    "\n",
    "enet_cv = GridSearchCV(enet, cv=5,  param_grid=param_grid, scoring=balanced_acc)\n",
    "%time scores = cross_val_score(estimator=enet_cv, X=X, y=y, cv=5,\\\n",
    "    scoring=balanced_acc, n_jobs=-1)\n",
    "print(\"Test bACC:%.2f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
