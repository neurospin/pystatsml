{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn processing pipelines\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "Sources: http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html\n",
    "\n",
    "### Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(pd.get_dummies(['A', 'B', 'C', 'A', 'B', 'D']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization of input features\n",
    "\n",
    "Sources:\n",
    "\n",
    "- http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "- http://stats.stackexchange.com/questions/111017/question-about-standardizing-in-ridge-regression\n",
    "\n",
    "\n",
    "\"Standardizing\" or mean removal and variance scaling, is not systematic. For example multiple linear regression does not require it. However it is a good practice in many cases:\n",
    "\n",
    "- The **variable combination method is sensitive scales**. If the input variables are combined via a distance function (such as Euclidean distance) in an RBF network, standardizing inputs can be crucial. The contribution of an input will depend heavily on its variability relative to other inputs. If one input has a range of 0 to 1, while another input has a range of 0 to 1,000,000, then the contribution of the first input to the distance will be swamped by the second input.\n",
    "\n",
    "- **Regularized learning algorithm**. Lasso or Ridge regression regularize the linear regression by imposing a penalty on the size of coefficients. Thus the coefficients are shrunk toward zero and toward each other. But when this happens and if the independent variables does not have the same scale, the shrinking is not fair. Two independent variables with different scales will have different contributions to the penalized terms, because the penalized term is norm (a sum of squares, or absolute values) of all the coefficients. To avoid such kind of problems, very often, the independent variables are centered and scaled in order to have variance 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# dataset\n",
    "np.random.seed(42)\n",
    "n_samples, n_features, n_features_info = 100, 5, 3\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "beta = np.zeros(n_features)\n",
    "beta[:n_features_info] = 1\n",
    "Xbeta = np.dot(X, beta)\n",
    "eps = np.random.randn(n_samples)\n",
    "y = Xbeta + eps\n",
    "\n",
    "X[:, 0] *= 1e6  # inflate the first feature\n",
    "X[:, 1] += 1e6  # bias the second feature\n",
    "y = 100 * y + 1000  # bias and scale the output\n",
    "\n",
    "print(\"== Linear regression: scaling is not required ==\")\n",
    "model = lm.LinearRegression()\n",
    "model.fit(X, y)\n",
    "print(\"Coefficients:\", model.coef_, model.intercept_)\n",
    "print(\"Test R2:%.2f\" % cross_val_score(estimator=model, X=X, y=y, cv=5).mean())\n",
    "\n",
    "print(\"== Lasso without scaling ==\")\n",
    "model = lm.LassoCV(cv=3)\n",
    "model.fit(X, y)\n",
    "print(\"Coefficients:\", model.coef_, model.intercept_)\n",
    "print(\"Test R2:%.2f\" % cross_val_score(estimator=model, X=X, y=y, cv=5).mean())\n",
    "\n",
    "print(\"== Lasso with scaling ==\")\n",
    "model = lm.LassoCV(cv=3)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "Xc = scaler.fit(X).transform(X)\n",
    "model.fit(Xc, y)\n",
    "print(\"Coefficients:\", model.coef_, model.intercept_)\n",
    "print(\"Test R2:%.2f\" % cross_val_score(estimator=model, X=Xc, y=y, cv=5).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn pipelines\n",
    "\n",
    "Sources: http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "Note that statistics such as the mean and standard deviation are computed from the training data, not from the validation or test data. The validation and test data must be standardized using the statistics computed from the training data. Thus Standardization should be merged together with the learner using a ``Pipeline``.\n",
    "\n",
    "Pipeline chain multiple estimators into one. All estimators in a pipeline, except the last one, must have the ``fit()`` and ``transform()`` methods. The last must implement the ``fit()`` and ``predict()`` methods.\n",
    "\n",
    "### Standardization of input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import sklearn.linear_model as lm\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "model = make_pipeline(preprocessing.StandardScaler(), lm.LassoCV(cv=3))\n",
    "\n",
    "# or\n",
    "from sklearn.pipeline import Pipeline\n",
    "model = Pipeline([('standardscaler', preprocessing.StandardScaler()), \n",
    "                  ('lassocv', lm.LassoCV(cv=3))])\n",
    "\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=5)\n",
    "print(\"Test  r2:%.2f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features selection\n",
    "\n",
    "An alternative to features selection based on $\\ell_1$ penalty is to use a preprocessing stp of univariate feature selection.\n",
    "\n",
    "Such methods, called **filters**, are a simple, widely used method for supervised dimension reduction [26]. Filters are univariate methods that rank features according to their ability to predict the target, independently of other features. This ranking may be based on parametric (e.g., t-tests) or nonparametric (e.g., Wilcoxon tests) statistical methods. Filters are computationally efficient and more robust to overfitting than multivariate methods. However, they are blind to feature interrelations, a problem that can be addressed only with multivariate selection such as learning with $\\ell_1$ penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples, n_features, n_features_info = 100, 100, 3\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "beta = np.zeros(n_features)\n",
    "beta[:n_features_info] = 1\n",
    "Xbeta = np.dot(X, beta)\n",
    "eps = np.random.randn(n_samples)\n",
    "y = Xbeta + eps\n",
    "\n",
    "X[:, 0] *= 1e6  # inflate the first feature\n",
    "X[:, 1] += 1e6  # bias the second feature\n",
    "y = 100 * y + 1000  # bias and scale the output\n",
    "\n",
    "model = Pipeline([('anova', SelectKBest(f_regression, k=3)),\n",
    "                  ('lm', lm.LinearRegression())])\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=5)\n",
    "print(\"Anova filter + linear regression, test  r2:%.2f\" % scores.mean())\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "model = Pipeline([('standardscaler', preprocessing.StandardScaler()),\n",
    "                  ('lassocv', lm.LassoCV(cv=3))])\n",
    "scores = cross_val_score(estimator=model, X=X, y=y, cv=5)\n",
    "print(\"Standardize + Lasso, test  r2:%.2f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression pipelines with CV for parameters selection\n",
    "\n",
    "Now we combine standardization of input features, feature selection and learner with hyper-parameter within a pipeline which is warped in a grid search procedure to select the best hyperparameters based on a (inner)CV. The overall is plugged in an outer CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Datasets\n",
    "n_samples, n_features, noise_sd = 100, 100, 20\n",
    "X, y, coef = datasets.make_regression(n_samples=n_samples, n_features=n_features, \n",
    "                                      noise=noise_sd, n_informative=5,\n",
    "                                      random_state=42, coef=True)\n",
    " \n",
    "# Use this to tune the noise parameter such that snr < 5\n",
    "print(\"SNR:\", np.std(np.dot(X, coef)) / noise_sd)\n",
    "\n",
    "print(\"=============================\")\n",
    "print(\"== Basic linear regression ==\")\n",
    "print(\"=============================\")\n",
    "\n",
    "scores = cross_val_score(estimator=lm.LinearRegression(), X=X, y=y, cv=5)\n",
    "print(\"Test  r2:%.2f\" % scores.mean())\n",
    "\n",
    "print(\"==============================================\")\n",
    "print(\"== Scaler + anova filter + ridge regression ==\")\n",
    "print(\"==============================================\")\n",
    "\n",
    "anova_ridge = Pipeline([\n",
    "    ('standardscaler', preprocessing.StandardScaler()),\n",
    "    ('selectkbest', SelectKBest(f_regression)),\n",
    "    ('ridge', lm.Ridge())\n",
    "])\n",
    "param_grid = {'selectkbest__k':np.arange(10, 110, 10), \n",
    "              'ridge__alpha':[.001, .01, .1, 1, 10, 100] }\n",
    "\n",
    "# Expect execution in ipython, for python remove the %time\n",
    "print(\"----------------------------\")\n",
    "print(\"-- Parallelize inner loop --\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "anova_ridge_cv = GridSearchCV(anova_ridge, cv=5,  param_grid=param_grid, n_jobs=-1)\n",
    "%time scores = cross_val_score(estimator=anova_ridge_cv, X=X, y=y, cv=5)\n",
    "print(\"Test r2:%.2f\" % scores.mean())\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"-- Parallelize outer loop --\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "anova_ridge_cv = GridSearchCV(anova_ridge, cv=5,  param_grid=param_grid)\n",
    "%time scores = cross_val_score(estimator=anova_ridge_cv, X=X, y=y, cv=5, n_jobs=-1)\n",
    "print(\"Test r2:%.2f\" % scores.mean())\n",
    "\n",
    "\n",
    "print(\"=====================================\")\n",
    "print(\"== Scaler + Elastic-net regression ==\")\n",
    "print(\"=====================================\")\n",
    "\n",
    "alphas = [.0001, .001, .01, .1, 1, 10, 100, 1000] \n",
    "l1_ratio = [.1, .5, .9]\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"-- Parallelize outer loop --\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "enet = Pipeline([\n",
    "    ('standardscaler', preprocessing.StandardScaler()),\n",
    "    ('enet', lm.ElasticNet(max_iter=10000)),\n",
    "])\n",
    "param_grid = {'enet__alpha':alphas ,\n",
    "              'enet__l1_ratio':l1_ratio}\n",
    "enet_cv = GridSearchCV(enet, cv=5,  param_grid=param_grid)\n",
    "%time scores = cross_val_score(estimator=enet_cv, X=X, y=y, cv=5, n_jobs=-1)\n",
    "print(\"Test r2:%.2f\" % scores.mean())\n",
    "\n",
    "print(\"-----------------------------------------------\")\n",
    "print(\"-- Parallelize outer loop + built-in CV      --\")\n",
    "print(\"-- Remark: scaler is only done on outer loop --\")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "enet_cv = Pipeline([\n",
    "    ('standardscaler', preprocessing.StandardScaler()),\n",
    "    ('enet', lm.ElasticNetCV(max_iter=10000, l1_ratio=l1_ratio, alphas=alphas, cv=3)),\n",
    "])\n",
    "\n",
    "%time scores = cross_val_score(estimator=enet_cv, X=X, y=y, cv=5)\n",
    "print(\"Test r2:%.2f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification pipelines with CV for parameters selection\n",
    "\n",
    "Now we combine standardization of input features, feature selection and learner with hyper-parameter within a pipeline which is warped in a grid search procedure to select the best hyperparameters based on a (inner)CV. The overall is plugged in an outer CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "    \n",
    "# Datasets\n",
    "n_samples, n_features, noise_sd = 100, 100, 20\n",
    "X, y = datasets.make_classification(n_samples=n_samples, n_features=n_features,\n",
    "                         n_informative=5, random_state=42)\n",
    "\n",
    "\n",
    "def balanced_acc(estimator, X, y, **kwargs):\n",
    "    '''\n",
    "    Balanced acuracy scorer\n",
    "    '''\n",
    "    return metrics.recall_score(y, estimator.predict(X), average=None).mean()\n",
    "\n",
    "print(\"=============================\")\n",
    "print(\"== Basic logistic regression ==\")\n",
    "print(\"=============================\")\n",
    "\n",
    "scores = cross_val_score(estimator=lm.LogisticRegression(C=1e8, \n",
    "                                   class_weight='balanced',\n",
    "                                   solver='lbfgs'),\n",
    "                         X=X, y=y, cv=5, scoring=balanced_acc)\n",
    "print(\"Test  bACC:%.2f\" % scores.mean())\n",
    "\n",
    "print(\"=======================================================\")\n",
    "print(\"== Scaler + anova filter + ridge logistic regression ==\")\n",
    "print(\"=======================================================\")\n",
    "\n",
    "anova_ridge = Pipeline([\n",
    "    ('standardscaler', preprocessing.StandardScaler()),\n",
    "    ('selectkbest', SelectKBest(f_classif)),\n",
    "    ('ridge', lm.LogisticRegression(penalty='l2', \n",
    "                                    class_weight='balanced',\n",
    "                                   solver='lbfgs'))\n",
    "])\n",
    "param_grid = {'selectkbest__k':np.arange(10, 110, 10), \n",
    "              'ridge__C':[.0001, .001, .01, .1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "\n",
    "# Expect execution in ipython, for python remove the %time\n",
    "print(\"----------------------------\")\n",
    "print(\"-- Parallelize inner loop --\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "anova_ridge_cv = GridSearchCV(anova_ridge, cv=5,  param_grid=param_grid, \n",
    "                              scoring=balanced_acc, n_jobs=-1)\n",
    "%time scores = cross_val_score(estimator=anova_ridge_cv, X=X, y=y, cv=5,\\\n",
    "                               scoring=balanced_acc)\n",
    "print(\"Test bACC:%.2f\" % scores.mean())\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"-- Parallelize outer loop --\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "anova_ridge_cv = GridSearchCV(anova_ridge, cv=5,  param_grid=param_grid,\n",
    "                              scoring=balanced_acc)\n",
    "%time scores = cross_val_score(estimator=anova_ridge_cv, X=X, y=y, cv=5,\\\n",
    "                               scoring=balanced_acc, n_jobs=-1)\n",
    "print(\"Test bACC:%.2f\" % scores.mean())\n",
    "\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"== Scaler + lasso logistic regression ==\")\n",
    "print(\"========================================\")\n",
    "\n",
    "Cs = np.array([.0001, .001, .01, .1, 1, 10, 100, 1000, 10000])\n",
    "alphas = 1 / Cs\n",
    "l1_ratio = [.1, .5, .9]\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"-- Parallelize outer loop --\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "lasso = Pipeline([\n",
    "    ('standardscaler', preprocessing.StandardScaler()),\n",
    "    ('lasso', lm.LogisticRegression(penalty='l1', \n",
    "                                    class_weight='balanced')),\n",
    "])\n",
    "param_grid = {'lasso__C':Cs}\n",
    "enet_cv = GridSearchCV(lasso, cv=5,  param_grid=param_grid, scoring=balanced_acc)\n",
    "%time scores = cross_val_score(estimator=enet_cv, X=X, y=y, cv=5,\\\n",
    "                               scoring=balanced_acc, n_jobs=-1)\n",
    "print(\"Test bACC:%.2f\" % scores.mean())\n",
    "\n",
    "\n",
    "print(\"-----------------------------------------------\")\n",
    "print(\"-- Parallelize outer loop + built-in CV      --\")\n",
    "print(\"-- Remark: scaler is only done on outer loop --\")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "lasso_cv = Pipeline([\n",
    "    ('standardscaler', preprocessing.StandardScaler()),\n",
    "    ('lasso', lm.LogisticRegressionCV(Cs=Cs, scoring=balanced_acc)),\n",
    "])\n",
    "\n",
    "%time scores = cross_val_score(estimator=lasso_cv, X=X, y=y, cv=5)\n",
    "print(\"Test bACC:%.2f\" % scores.mean())\n",
    "\n",
    "\n",
    "print(\"=============================================\")\n",
    "print(\"== Scaler + Elasticnet logistic regression ==\")\n",
    "print(\"=============================================\")\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"-- Parallelize outer loop --\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "enet = Pipeline([\n",
    "    ('standardscaler', preprocessing.StandardScaler()),\n",
    "    ('enet', lm.SGDClassifier(loss=\"log\", penalty=\"elasticnet\",\n",
    "                            alpha=0.0001, l1_ratio=0.15, class_weight='balanced')),\n",
    "])\n",
    "\n",
    "param_grid = {'enet__alpha':alphas,\n",
    "              'enet__l1_ratio':l1_ratio}\n",
    "\n",
    "enet_cv = GridSearchCV(enet, cv=5,  param_grid=param_grid, scoring=balanced_acc)\n",
    "%time scores = cross_val_score(estimator=enet_cv, X=X, y=y, cv=5,\\\n",
    "    scoring=balanced_acc, n_jobs=-1)\n",
    "print(\"Test bACC:%.2f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
