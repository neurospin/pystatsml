{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction and feature extraction\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In machine learning and statistics, dimensionality reduction or dimension reduction is the process of reducing the number of features under consideration, and can be divided into feature selection (not addressed here) and feature extraction.\n",
    "\n",
    "Feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is related to dimensionality reduction.\n",
    "\n",
    "The input matrix $\\mathbf{X}$, of dimension $N \\times P$, is\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{11} & \\ldots     &     x_{1P}\\\\\n",
    "       &            &      \\\\\n",
    "\\vdots & \\mathbf{X} & \\vdots\\\\\n",
    "       &            &      \\\\\n",
    "x_{N1} & \\ldots     &     x_{NP}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where the rows represent the samples and columns represent the variables.\n",
    "\n",
    "\n",
    "The goal is to learn a transformation that extracts a few relevant features. This is generally done by exploiting the covariance $\\mathbf{\\Sigma_{XX}}$ between the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular value decomposition and matrix factorization\n",
    "\n",
    "### Matrix factorization principles\n",
    "\n",
    "Decompose the data matrix $\\mathbf{X}_{N \\times P}$ into a product of a mixing matrix $\\mathbf{U}_{N \\times K}$ and a dictionary matrix $\\mathbf{V}_{P \\times K}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\mathbf{U} \\mathbf{V}^{T},\n",
    "$$\n",
    "\n",
    "If we consider only a subset of components $K<rank(\\mathbf{X}) < \\min(P, N-1)$ , $\\mathbf{X}$ is approximated by a matrix $\\hat{\\mathbf{X}}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} \\approx \\hat{\\mathbf{X}} = \\mathbf{U} \\mathbf{V}^{T},\n",
    "$$\n",
    "\n",
    "Each line of $\\mathbf{x_i}$ is a linear combination (mixing $\\mathbf{u_i}$) of dictionary items $\\mathbf{V}$.\n",
    "\n",
    "$N$ $P$-dimensional data points lie in a space whose dimension is less than $N-1$ (2 dots lie on a line, 3 on a plane, etc.).\n",
    "\n",
    "![Matrix factorization](images/svd_mixing_dict.png)\n",
    "\n",
    "\n",
    "### Singular value decomposition (SVD) principles\n",
    "\n",
    "Singular-value decomposition (SVD) factorises the data matrix $\\mathbf{X}_{N \\times P}$ into a product:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\mathbf{U}\\mathbf{D}\\mathbf{V}^{T},\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{11} &            & x_{1P}\\\\\n",
    "       &            &       \\\\\n",
    "       & \\mathbf{X} &       \\\\\n",
    "       &            &       \\\\\n",
    "x_{N1} &            & x_{NP}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "u_{11} &            & u_{1K}\\\\\n",
    "       &            &       \\\\\n",
    "       & \\mathbf{U} &       \\\\\n",
    "       &            &       \\\\\n",
    "u_{N1} &            & u_{NK}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "d_{1}&            & 0\\\\\n",
    "     & \\mathbf{D} &\\\\\n",
    " 0   &            & d_{K}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v_{11}&              & v_{1P}\\\\\n",
    "      & \\mathbf{V}^T &       \\\\\n",
    "v_{K1}&              & v_{KP}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "$\\mathbf{U}$: **right-singular**\n",
    "\n",
    "- $\\mathbf{V} = [\\mathbf{v}_1,\\cdots , \\mathbf{v}_K]$ is a $P \\times K$ orthogonal matrix.\n",
    "\n",
    "- It is a **dictionary** of patterns to be combined (according to the mixing coefficients) to reconstruct the original samples.\n",
    "\n",
    "- $\\mathbf{V}$ perfoms the initial **rotations** (**projection**) along the $K=\\min(N, P)$ **principal component directions**, also called **loadings**.\n",
    "\n",
    "- Each $\\mathbf{v}_j$ performs the linear combination of the variables that has maximum sample variance, subject to being uncorrelated with the previous $\\mathbf{v}_{j-1}$.\n",
    "\n",
    "\n",
    "$\\mathbf{D}$: **singular values**\n",
    "\n",
    "- $\\mathbf{D}$ is a $K \\times K$ diagonal matrix made of the singular values of $\\mathbf{X}$ with $d_1 \\geq d_2 \\geq \\cdots \\geq d_K \\geq 0$.\n",
    "\n",
    "- $\\mathbf{D}$ scale the projection along the coordinate axes by $d_1, d_2, \\cdots, d_K$.\n",
    "\n",
    "- Singular values are the square roots of the eigenvalues of $\\mathbf{X}^{T}\\mathbf{X}$.\n",
    "\n",
    "\n",
    "$\\mathbf{V}$: **left-singular vectors**\n",
    "\n",
    "- $\\mathbf{U} = [\\mathbf{u}_1, \\cdots , \\mathbf{u}_K]$ is an $N \\times K$ orthogonal matrix.\n",
    "\n",
    "- Each row $\\mathbf{v_i}$ provides the **mixing coefficients** of dictionary items to reconstruct the sample $\\mathbf{x_i}$\n",
    "\n",
    "- It may be understood as the coordinates on the new orthogonal basis (obtained after the initial rotation) called **principal components** in the PCA. \n",
    "\n",
    "\n",
    "### SVD for variables transformation\n",
    "\n",
    "$\\mathbf{V}$ transforms correlated variables ($\\mathbf{X}$) into a set of uncorrelated ones ($\\mathbf{U}\\mathbf{D}$) that better expose the various relationships among the original data items.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{X}           &= \\mathbf{U}\\mathbf{D}\\mathbf{V}^{T},\\\\\n",
    "\\mathbf{X}\\mathbf{V} &= \\mathbf{U}\\mathbf{D}\\mathbf{V}^{T}\\mathbf{V},\\\\\n",
    "\\mathbf{X}\\mathbf{V} &= \\mathbf{U}\\mathbf{D}\\mathbf{I},\\\\\n",
    "\\mathbf{X}\\mathbf{V} &= \\mathbf{U}\\mathbf{D}\n",
    "\\end{align}\n",
    "\n",
    "At the same time, SVD is a method for identifying and ordering the dimensions along which data points exhibit the most variation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:53:10.303257Z",
     "iopub.status.busy": "2020-10-11T22:53:10.302852Z",
     "iopub.status.idle": "2020-10-11T22:53:11.253464Z",
     "shell.execute_reply": "2020-10-11T22:53:11.253886Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    " \n",
    "# dataset\n",
    "n_samples = 100\n",
    "experience = np.random.normal(size=n_samples)\n",
    "salary = 1500 + experience + np.random.normal(size=n_samples, scale=.5)\n",
    "X = np.column_stack([experience, salary])\n",
    "\n",
    "# PCA using SVD\n",
    "X -= X.mean(axis=0)  # Centering is required\n",
    "U, s, Vh = scipy.linalg.svd(X, full_matrices=False)\n",
    "# U : Unitary matrix having left singular vectors as columns.\n",
    "#     Of shape (n_samples,n_samples) or (n_samples,n_comps), depending on\n",
    "#     full_matrices.\n",
    "#\n",
    "# s : The singular values, sorted in non-increasing order. Of shape (n_comps,), \n",
    "#     with n_comps = min(n_samples, n_features).\n",
    "#\n",
    "# Vh: Unitary matrix having right singular vectors as rows. \n",
    "#     Of shape (n_features, n_features) or (n_comps, n_features) depending \n",
    "# on full_matrices.\n",
    "\n",
    "plt.figure(figsize=(9, 3)) \n",
    "\n",
    "plt.subplot(131)\n",
    "plt.scatter(U[:, 0], U[:, 1], s=50)\n",
    "plt.axis('equal')\n",
    "plt.title(\"U: Rotated and scaled data\")\n",
    "\n",
    "plt.subplot(132)\n",
    "\n",
    "# Project data\n",
    "PC = np.dot(X, Vh.T)\n",
    "plt.scatter(PC[:, 0], PC[:, 1], s=50)\n",
    "plt.axis('equal')\n",
    "plt.title(\"XV: Rotated data\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "for i in range(Vh.shape[0]):\n",
    "    plt.arrow(x=0, y=0, dx=Vh[i, 0], dy=Vh[i, 1], head_width=0.2, \n",
    "              head_length=0.2, linewidth=2, fc='r', ec='r')\n",
    "    plt.text(Vh[i, 0], Vh[i, 1],'v%i' % (i+1), color=\"r\", fontsize=15,\n",
    "             horizontalalignment='right', verticalalignment='top')\n",
    "plt.axis('equal')\n",
    "plt.ylim(-4, 4)\n",
    "\n",
    "plt.title(\"X: original data (v1, v2:PC dir.)\")\n",
    "plt.xlabel(\"experience\")\n",
    "plt.ylabel(\"salary\")\n",
    "           \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal components analysis (PCA)\n",
    "\n",
    "Sources:\n",
    "\n",
    "- C. M. Bishop *Pattern Recognition and Machine Learning*, Springer, 2006\n",
    "\n",
    "- [Everything you did and didn't know about PCA](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/)\n",
    "\n",
    "- [Principal Component Analysis in 3 Simple Steps](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html)\n",
    "\n",
    "\n",
    "### Principles\n",
    "\n",
    "- Principal components analysis is the main method used for linear dimension reduction.\n",
    "\n",
    "- The idea of principal component analysis is to find the $K$ **principal components directions** (called the **loadings**) $\\mathbf{V}_{K\\times P}$ that capture the variation in the data as much as possible.\n",
    "\n",
    "- It converts a set of $N$ $P$-dimensional observations $\\mathbf{N}_{N\\times P}$ of possibly correlated variables into a set of $N$ $K$-dimensional samples  $\\mathbf{C}_{N\\times K}$, where the $K < P$. The new variables are linearly uncorrelated. The columns of $\\mathbf{C}_{N\\times K}$ are called the **principal components**.\n",
    "\n",
    "- The dimension reduction is obtained by using only $K < P$ components that exploit correlation (covariance) among the original variables.\n",
    "\n",
    "- PCA is mathematically defined as an orthogonal linear transformation $\\mathbf{V}_{K\\times P}$ that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.\n",
    "$$\n",
    "\\mathbf{C}_{N\\times K} = \\mathbf{X}_{N \\times P} \\mathbf{V}_{P \\times K} \n",
    "$$\n",
    "\n",
    "- PCA can be thought of as fitting a $P$-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipse is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only a commensurately small amount of information.\n",
    "\n",
    "- Finding the $K$ largest axes of the ellipse will permit to project the data onto a space having dimensionality $K < P$ while maximizing the variance of the projected data.\n",
    "\n",
    "### Dataset preprocessing\n",
    "\n",
    "#### Centering\n",
    "\n",
    "Consider a data matrix, $\\mathbf{X}$ , with column-wise zero empirical mean (the sample mean of each column has been shifted to zero), ie. $\\mathbf{X}$ is replaced by $\\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^T$.\n",
    "\n",
    "#### Standardizing\n",
    "\n",
    "Optionally, standardize the columns, i.e., scale them by their standard-deviation. Without standardization, a variable with a high variance will capture most of the effect of the PCA. The principal direction will be aligned with this variable. Standardization will, however, raise noise variables to the save level as informative variables.\n",
    "\n",
    "The covariance matrix of centered standardized data is the correlation matrix.\n",
    "\n",
    "### Eigendecomposition of the data covariance matrix\n",
    "\n",
    "To begin with, consider the projection onto a one-dimensional space ($K = 1$). We can define the direction of this space using a $P$-dimensional vector $\\mathbf{v}$, which for convenience (and without loss of generality) we shall choose to be a unit vector so that $\\|\\mathbf{v}\\|_2 = 1$ (note that we are only interested in the direction defined by $\\mathbf{v}$, not in the magnitude of $\\mathbf{v}$ itself). PCA consists of two mains steps:\n",
    "\n",
    "**Projection in the directions that capture the greatest variance**\n",
    "\n",
    "Each $P$-dimensional data point $\\mathbf{x}_i$ is then projected onto $\\mathbf{v}$, where the coordinate (in the coordinate system of $\\mathbf{v}$) is a scalar value, namely $\\mathbf{x}_i^T \\mathbf{v}$. I.e., we want to find the vector $\\mathbf{v}$ that maximizes these coordinates along $\\mathbf{v}$, which we will see corresponds to maximizing the variance of the projected data. This is equivalently expressed as\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = \\arg \\max_{\\|\\mathbf{v}\\|=1}\\frac{1}{N}\\sum_i \\left(\\mathbf{x}_i^T \\mathbf{v}\\right)^2.\n",
    "$$\n",
    "\n",
    "We can write this in matrix form as\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = \\arg \\max_{\\|\\mathbf{v}\\|=1} \\frac{1}{N} \\|\\mathbf{X} \\mathbf{v}\\|^2 = \\frac{1}{N} \\mathbf{v}^T \\mathbf{X}^T \\mathbf{X} \\mathbf{v} = \\mathbf{v}^T\\mathbf{S_{XX}}\\mathbf{v},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{S_{XX}}$ is a biased estiamte of the covariance matrix of the data, i.e.\n",
    "\n",
    "$$\n",
    "\\mathbf{S_{XX}} = \\frac{1}{N} \\mathbf{X}^T\\mathbf{X}.\n",
    "$$\n",
    "\n",
    "We now maximize the projected variance $\\mathbf{v}^T \\mathbf{S_{XX}} \\mathbf{v}$ with respect to $\\mathbf{v}$. Clearly, this has to be a constrained maximization to prevent $\\|\\mathbf{v}_2\\| \\rightarrow \\infty$. The appropriate constraint comes from the normalization condition $\\|\\mathbf{v}\\|_2 \\equiv \\|\\mathbf{v}\\|_2^2 = \\mathbf{v}^T \\mathbf{v} = 1$. To enforce this constraint, we introduce a [Lagrange multiplier](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/constrained-optimization/a/lagrange-multipliers-single-constraint) that we shall denote by $\\lambda$, and then make an unconstrained maximization of\n",
    "\n",
    "$$\n",
    "\\mathbf{v}^T\\mathbf{S_{XX}} \\mathbf{v} - \\lambda (\\mathbf{v}^T \\mathbf{v} - 1).\n",
    "$$\n",
    "\n",
    "By setting the gradient with respect to $\\mathbf{v}$ equal to zero, we see that this quantity has a stationary\n",
    "point when\n",
    "\n",
    "$$\n",
    "\\mathbf{S_{XX}} \\mathbf{v} = \\lambda \\mathbf{v}.\n",
    "$$\n",
    "\n",
    "We note that $\\mathbf{v}$ is an eigenvector of $\\mathbf{S_{XX}}$.\n",
    "\n",
    "If we left-multiply the above equation by $\\mathbf{v}^T$ and make use of $\\mathbf{v}^T \\mathbf{v} = 1$, we see that the variance is given by\n",
    "\n",
    "$$\n",
    "\\mathbf{v}^T \\mathbf{S_{XX}} \\mathbf{v} = \\lambda,\n",
    "$$\n",
    "\n",
    "and so the variance will be at a maximum when $\\mathbf{v}$ is equal to the eigenvector corresponding to the largest eigenvalue, $\\lambda$. This eigenvector is known as the first principal component.\n",
    "\n",
    "We can define additional principal components in an incremental fashion by choosing each new direction to be that which maximizes the projected variance amongst all possible directions that are orthogonal to those already considered. If we consider the general case of a $K$-dimensional projection space, the optimal linear projection for which the variance of the projected data is maximized is now defined by the $K$ eigenvectors, $\\mathbf{v_1}, \\ldots , \\mathbf{v_K}$, of the data covariance matrix $\\mathbf{S_{XX}}$ that corresponds to the $K$ largest eigenvalues, $\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_K$.\n",
    "\n",
    "#### Back to SVD\n",
    "\n",
    "The sample covariance matrix of **centered data** $\\mathbf{X}$ is given by\n",
    "\n",
    "$$\n",
    "\\mathbf{S_{XX}} = \\frac{1}{N-1}\\mathbf{X}^T\\mathbf{X}.\n",
    "$$\n",
    "\n",
    "We rewrite $\\mathbf{X}^T\\mathbf{X}$ using the SVD decomposition of $\\mathbf{X}$ as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{X}^T\\mathbf{X}\n",
    "     &= (\\mathbf{U}\\mathbf{D}\\mathbf{V}^T)^T(\\mathbf{U}\\mathbf{D}\\mathbf{V}^T)\\\\\n",
    "     &= \\mathbf{V}\\mathbf{D}^T\\mathbf{U}^T\\mathbf{U}\\mathbf{D}\\mathbf{V}^T\\\\\n",
    "     &=\\mathbf{V}\\mathbf{D}^2\\mathbf{V}^T\\\\\n",
    "    \\mathbf{V}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{V} &= \\mathbf{D}^2\\\\\n",
    "    \\frac{1}{N-1} \\mathbf{V}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{V} &= \\frac{1}{N-1}\\mathbf{D}^2\\\\\n",
    "    \\mathbf{V}^T\\mathbf{S_{XX}}\\mathbf{V} &= \\frac{1}{N-1}\\mathbf{D}^2\n",
    "\\end{align*}.\n",
    "\n",
    "Considering only the $k^{th}$ right-singular vectors $\\mathbf{v}_k$ associated to the singular value $d_k$\n",
    "\n",
    "$$\n",
    "\\mathbf{v_k}^T\\mathbf{S_{XX}}\\mathbf{v_k} = \\frac{1}{N-1}d_k^2,\n",
    "$$\n",
    "\n",
    "It turns out that if you have done the singular value decomposition then you already have the Eigenvalue decomposition for $\\mathbf{X}^T\\mathbf{X}$. Where\n",
    "- The eigenvectors of $\\mathbf{S_{XX}}$ are equivalent to the right singular vectors, $\\mathbf{V}$, of $\\mathbf{X}$.\n",
    "- The eigenvalues, $\\lambda_k$, of $\\mathbf{S_{XX}}$, i.e. the variances of the components, are equal to $\\frac{1}{N-1}$ times the squared singular values, $d_k$.\n",
    "\n",
    "Moreover computing PCA with SVD do not require to form the matrix $\\mathbf{X}^T\\mathbf{X}$, so computing the SVD is now the standard way to calculate a principal components analysis from a data matrix, unless only a handful of components are required.\n",
    "\n",
    "#### PCA outputs\n",
    "\n",
    "The SVD or the eigendecomposition of the data covariance matrix provides three main quantities:\n",
    "\n",
    "1. **Principal component directions** or **loadings** are the **eigenvectors** of $\\mathbf{X}^T\\mathbf{X}$. The $\\mathbf{V}_{K \\times P}$ or the **right-singular vectors** of an SVD of $\\mathbf{X}$ are called principal component directions of $\\mathbf{X}$. They are generally computed using the SVD of $\\mathbf{X}$.\n",
    "\n",
    "2. **Principal components** is the ${N\\times K}$ matrix $\\mathbf{C}$ which is obtained by projecting $\\mathbf{X}$ onto the principal components directions, i.e.\n",
    "\n",
    "$$\n",
    "    \\mathbf{C}_{N\\times K} = \\mathbf{X}_{N \\times P} \\mathbf{V}_{P \\times K}.\n",
    "$$\n",
    "\n",
    "Since $\\mathbf{X} = \\mathbf{UDV}^T$ and $\\mathbf{V}$ is orthogonal ($\\mathbf{V}^T \\mathbf{V} = \\mathbf{I}$): \n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{C}_{N\\times K} &= \\mathbf{UDV}^T_{N \\times P} \\mathbf{V}_{P \\times K}\\\\ \n",
    "    \\mathbf{C}_{N\\times K} &= \\mathbf{UD}^T_{N \\times K} \\mathbf{I}_{K \\times K}\\\\\n",
    "    \\mathbf{C}_{N\\times K} &= \\mathbf{UD}^T_{N \\times K}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Thus $\\mathbf{c}_j = \\mathbf{X}\\mathbf{v}_j = \\mathbf{u}_j d_j$, for $j=1, \\ldots K$. Hence $\\mathbf{u}_j$ is simply the projection of the row vectors of $\\mathbf{X}$, i.e., the input predictor vectors, on the direction $\\mathbf{v}_j$, scaled by $d_j$.\n",
    "\n",
    "$$\n",
    "\\mathbf{c}_1=\n",
    "\\begin{bmatrix}\n",
    "x_{1,1}v_{1,1}+ \\ldots +x_{1,P}v_{1,P}\\\\\n",
    "x_{2,1}v_{1,1}+ \\ldots +x_{2,P}v_{1,P}\\\\\n",
    "\\vdots\\\\\n",
    "x_{N,1}v_{1,1}+ \\ldots +x_{N,P}v_{1,P}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. The **variance** of each component is given by the eigen values $\\lambda_k, k=1, \\dots K$. It can be obtained from the singular values:\n",
    "\n",
    "\\begin{align}\n",
    "var(\\mathbf{c}_k) =& \\frac{1}{N-1}(\\mathbf{X} \\mathbf{v}_k)^2\\\\\n",
    "                  =& \\frac{1}{N-1}(\\mathbf{u}_k d_k)^2\\\\\n",
    "                  =& \\frac{1}{N-1}d_k^2\n",
    "\\end{align}\n",
    "\n",
    "### Determining the number of PCs\n",
    "\n",
    "We must choose $K^* \\in [1, \\ldots,  K]$, the number of required components. This can be done by calculating the explained variance ratio of the $K^*$ first components and by choosing $K^*$ such that the **cumulative explained variance** ratio is greater than some given threshold (e.g., $\\approx 90\\%$). This is expressed as\n",
    "\n",
    "$$\n",
    "\\mathrm{cumulative~explained~variance}(\\mathbf{c}_k) = \\frac{\\sum_j^{K^*} var(\\mathbf{c}_k)}{\\sum_j^K var(\\mathbf{c}_k)}.\n",
    "$$\n",
    "\n",
    "### Interpretation and visualization\n",
    "\n",
    "**PCs**\n",
    "\n",
    "Plot the samples projeted on first the principal components as e.g. PC1 against PC2.\n",
    "\n",
    "**PC directions**\n",
    "\n",
    "Exploring the loadings associated with a component provides the contribution of each original variable in the component.\n",
    "\n",
    "Remark: The loadings (PC directions) are the coefficients of multiple regression of PC on original variables:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{c}                                             & = \\mathbf{X} \\mathbf{v}\\\\\n",
    "    \\mathbf{X}^T \\mathbf{c}                                & = \\mathbf{X}^T \\mathbf{X} \\mathbf{v}\\\\\n",
    "    (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{c} & = \\mathbf{v}\n",
    "\\end{align}\n",
    "\n",
    "Another way to evaluate the contribution of the original variables in each PC can be obtained by computing the correlation between the PCs and the original variables, i.e. columns of $\\mathbf{X}$, denoted $\\mathbf{x}_j$, for $j=1, \\ldots, P$. For the $k^{th}$ PC, compute and plot the correlations with all original variables\n",
    "\n",
    "$$\n",
    "cor(\\mathbf{c}_k, \\mathbf{x}_j),  j=1 \\ldots K, j=1 \\ldots K.\n",
    "$$\n",
    "\n",
    "These quantities are sometimes called the *correlation loadings*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:53:11.259044Z",
     "iopub.status.busy": "2020-10-11T22:53:11.258716Z",
     "iopub.status.idle": "2020-10-11T22:53:11.419610Z",
     "shell.execute_reply": "2020-10-11T22:53:11.419282Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    " \n",
    "# dataset\n",
    "n_samples = 100\n",
    "experience = np.random.normal(size=n_samples)\n",
    "salary = 1500 + experience + np.random.normal(size=n_samples, scale=.5)\n",
    "X = np.column_stack([experience, salary])\n",
    "\n",
    "# PCA with scikit-learn\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "PC = pca.transform(X)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(PC[:, 0], PC[:, 1])\n",
    "plt.xlabel(\"PC1 (var=%.2f)\" % pca.explained_variance_ratio_[0])\n",
    "plt.ylabel(\"PC2 (var=%.2f)\" % pca.explained_variance_ratio_[1])\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-dimensional Scaling (MDS)\n",
    "\n",
    "Resources:\n",
    "\n",
    "- http://www.stat.pitt.edu/sungkyu/course/2221Fall13/lec8_mds_combined.pdf\n",
    "- https://en.wikipedia.org/wiki/Multidimensional_scaling\n",
    "- Hastie, Tibshirani and Friedman (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction.* New York: Springer, Second Edition.\n",
    "\n",
    "The purpose of MDS is to find a low-dimensional projection of the data in which the pairwise distances between data points is preserved, as closely as possible (in a least-squares sense).\n",
    "\n",
    "- Let $\\mathbf{D}$ be the $(N \\times N)$ pairwise distance matrix where $d_{ij}$ is *a distance* between points $i$ and $j$.\n",
    "- The MDS concept can be extended to a wide variety of data types specified in terms of a similarity matrix.\n",
    "\n",
    "Given the dissimilarity (distance) matrix $\\mathbf{D}_{N \\times N}=[d_{ij}]$, MDS attempts to find  $K$-dimensional projections of the $N$ points $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\in \\mathbb{R}^K$, concatenated in an $\\mathbf{X}_{N \\times K}$ matrix, so that $d_{ij} \\approx \\|\\mathbf{x}_i - \\mathbf{x}_j\\|$ are as close as possible. This can be obtained by the minimization of a loss function called the **stress function**\n",
    "\n",
    "$$\n",
    "\\mathrm{stress}(\\mathbf{X}) = \\sum_{i \\neq j}\\left(d_{ij} -  \\|\\mathbf{x}_i - \\mathbf{x}_j\\|\\right)^2.\n",
    "$$\n",
    "\n",
    "This loss function is known as *least-squares* or *Kruskal-Shepard* scaling.\n",
    "\n",
    "A modification of *least-squares* scaling is the *Sammon mapping*\n",
    "\n",
    "$$\n",
    "\\mathrm{stress}_{\\mathrm{Sammon}}(\\mathbf{X}) = \\sum_{i \\neq j} \\frac{(d_{ij} -  \\|\\mathbf{x}_i - \\mathbf{x}_j\\|)^2}{d_{ij}}.\n",
    "$$\n",
    "\n",
    "The Sammon mapping performs better at preserving small distances compared to the *least-squares* scaling.\n",
    "\n",
    "### Classical multidimensional scaling\n",
    "\n",
    "Also known as *principal coordinates analysis*, PCoA.\n",
    "\n",
    "- The distance matrix, $\\mathbf{D}$, is transformed to a *similarity matrix*, $\\mathbf{B}$, often using centered inner products.\n",
    "\n",
    "- The loss function becomes\n",
    "\n",
    "$$\n",
    "\\mathrm{stress}_{\\mathrm{classical}}(\\mathbf{X}) = \\sum_{i \\neq j} \\big(b_{ij} -  \\langle\\mathbf{x}_i, \\mathbf{x}_j\\rangle\\big)^2.\n",
    "$$\n",
    "\n",
    "- The stress function in classical MDS is sometimes called *strain*.\n",
    "\n",
    "- The solution for the classical MDS problems can be found from the eigenvectors of the similarity matrix.\n",
    "\n",
    "- If the distances in $\\mathbf{D}$ are Euclidean and double centered inner products are used, the results are equivalent to PCA.\n",
    "\n",
    "### Example\n",
    "\n",
    "The ``eurodist`` datset provides the road distances (in kilometers) between 21 cities in Europe.\n",
    "Given this matrix of pairwise (non-Euclidean) distances $\\mathbf{D}=[d_{ij}]$, MDS can be used to recover the coordinates of the cities in *some* Euclidean referential whose orientation is arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:53:11.461678Z",
     "iopub.status.busy": "2020-10-11T22:53:11.461206Z",
     "iopub.status.idle": "2020-10-11T22:53:11.550662Z",
     "shell.execute_reply": "2020-10-11T22:53:11.550314Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pairwise distance between European cities\n",
    "try:\n",
    "    url = '../datasets/eurodist.csv'\n",
    "    df = pd.read_csv(url)\n",
    "except:\n",
    "    url = 'https://github.com/duchesnay/pystatsml/raw/master/datasets/eurodist.csv'\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "print(df.iloc[:5, :5])\n",
    "\n",
    "city = df[\"city\"]\n",
    "D = np.array(df.iloc[:, 1:])  # Distance matrix\n",
    "\n",
    "# Arbitrary choice of K=2 components\n",
    "from sklearn.manifold import MDS\n",
    "mds = MDS(dissimilarity='precomputed', n_components=2, random_state=40, max_iter=3000, eps=1e-9)\n",
    "X = mds.fit_transform(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recover coordinates of the cities in Euclidean referential whose orientation is arbitrary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:53:11.553745Z",
     "iopub.status.busy": "2020-10-11T22:53:11.553265Z",
     "iopub.status.idle": "2020-10-11T22:53:11.555476Z",
     "shell.execute_reply": "2020-10-11T22:53:11.555174Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "Deuclidean = metrics.pairwise.pairwise_distances(X, metric='euclidean')\n",
    "print(np.round(Deuclidean[:5, :5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:53:11.568797Z",
     "iopub.status.busy": "2020-10-11T22:53:11.568330Z",
     "iopub.status.idle": "2020-10-11T22:53:11.688507Z",
     "shell.execute_reply": "2020-10-11T22:53:11.688172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot: apply some rotation and flip\n",
    "theta = 80 * np.pi / 180.\n",
    "rot = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                [np.sin(theta),  np.cos(theta)]])\n",
    "Xr = np.dot(X, rot)\n",
    "# flip x\n",
    "Xr[:, 0] *= -1\n",
    "plt.scatter(Xr[:, 0], Xr[:, 1])\n",
    "\n",
    "for i in range(len(city)):\n",
    "    plt.text(Xr[i, 0], Xr[i, 1], city[i])\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the number of components\n",
    "\n",
    "We must choose $K^* \\in \\{1, \\ldots,  K\\}$ the number of required components. Plotting the values of the stress function, obtained using $k \\leq N-1$ components. In general, start with $1, \\ldots K \\leq 4$. Choose $K^*$ where you can clearly distinguish an *elbow* in the stress curve.\n",
    "\n",
    "Thus, in the plot below, we choose to retain information accounted for by the first *two* components, since this is where the *elbow* is in the stress curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:53:11.693707Z",
     "iopub.status.busy": "2020-10-11T22:53:11.692712Z",
     "iopub.status.idle": "2020-10-11T22:53:12.198962Z",
     "shell.execute_reply": "2020-10-11T22:53:12.199281Z"
    }
   },
   "outputs": [],
   "source": [
    "k_range = range(1, min(5, D.shape[0]-1))\n",
    "stress = [MDS(dissimilarity='precomputed', n_components=k,\n",
    "           random_state=42, max_iter=300, eps=1e-9).fit(D).stress_ for k in k_range]\n",
    "\n",
    "print(stress)\n",
    "plt.plot(k_range, stress)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"stress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear dimensionality reduction\n",
    "\n",
    "Sources:\n",
    "\n",
    "- [Scikit-learn documentation](http://scikit-learn.org/stable/modules/manifold.html)\n",
    "\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Isomap)\n",
    "\n",
    "Nonlinear dimensionality reduction or **manifold learning**  cover unsupervised methods that attempt to identify low-dimensional manifolds within the original $P$-dimensional space that represent high data density. Then those methods provide a mapping from the high-dimensional space to the low-dimensional embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isomap \n",
    "\n",
    "Isomap is a nonlinear dimensionality reduction method that combines a procedure to compute the distance matrix with MDS. The distances calculation is based on geodesic distances evaluated on neighborhood graph:\n",
    "\n",
    "1. Determine the neighbors of each point. All points in some fixed radius or K nearest neighbors.\n",
    "\n",
    "2. Construct a neighborhood graph. Each point is connected to other if it is a K nearest neighbor. Edge length equal to Euclidean distance.\n",
    "\n",
    "3. Compute shortest path between pairwise of points $d_{ij}$ to build the distance matrix $\\mathbf{D}$. \n",
    "\n",
    "4. Apply MDS on  $\\mathbf{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-11T22:53:12.204064Z",
     "iopub.status.busy": "2020-10-11T22:53:12.203634Z",
     "iopub.status.idle": "2020-10-11T22:53:12.793580Z",
     "shell.execute_reply": "2020-10-11T22:53:12.793933Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import manifold, datasets\n",
    "\n",
    "X, color = datasets.make_s_curve(1000, random_state=42)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.suptitle(\"Isomap Manifold Learning\", fontsize=14)\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
    "ax.view_init(4, -72)\n",
    "plt.title('2D \"S shape\" manifold in 3D')\n",
    "\n",
    "Y = manifold.Isomap(n_neighbors=10, n_components=2).fit_transform(X)\n",
    "ax = fig.add_subplot(122)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"Isomap\")\n",
    "plt.xlabel(\"First component\")\n",
    "plt.ylabel(\"Second component\")\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### PCA\n",
    "\n",
    "#### Write a basic PCA class\n",
    "\n",
    "Write a class `BasicPCA` with two methods:\n",
    "\n",
    "- `fit(X)` that estimates the data mean, principal components directions $\\textbf{V}$ and the explained variance of each component.\n",
    "\n",
    "- `transform(X)` that projects the data onto the principal components.\n",
    "\n",
    "Check that your `BasicPCA` gave similar results, compared to the results from `sklearn`.\n",
    "\n",
    "#### Apply your Basic PCA on the `iris` dataset\n",
    "\n",
    "The data set is available at: https://github.com/duchesnay/pystatsml/raw/master/datasets/iris.csv\n",
    "\n",
    "- Describe the data set. Should the dataset been standardized?\n",
    "\n",
    "- Describe the structure of correlations among variables.\n",
    "\n",
    "- Compute a PCA with the maximum number of components.\n",
    "\n",
    "- Compute the cumulative explained variance ratio. Determine the number of components $K$ by your computed values.\n",
    "\n",
    "- Print the $K$ principal components directions and correlations of the $K$ principal components with the original variables. Interpret the contribution of the original variables into the PC.\n",
    "\n",
    "- Plot the samples projected into the $K$ first PCs.\n",
    "\n",
    "- Color samples by their species.\n",
    "\n",
    "### MDS\n",
    "\n",
    "Apply MDS from `sklearn` on the `iris` dataset available at: \n",
    "\n",
    "https://github.com/duchesnay/pystatsml/raw/master/datasets/iris.csv\n",
    "\n",
    "- Center and scale the dataset.\n",
    "\n",
    "- Compute Euclidean pairwise distances matrix.\n",
    "\n",
    "- Select the number of components.\n",
    "\n",
    "- Show that classical MDS on Euclidean pairwise distances matrix is equivalent to PCA."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
