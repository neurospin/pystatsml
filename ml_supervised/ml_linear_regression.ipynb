{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear methods for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary least squares\n",
    "\n",
    "Linear regression models the **output**, or **target** variable $y \\in \\mathrm{R}$ as a linear combination of the $(P-1)$-dimensional input $x \\in \\mathrm{R}^{(P-1)}$. Let $\\mathbf{X}$ be the $N \\times P$ matrix with each row an input vector (with a 1 in the first position), and similarly let $y$ be the $N$-dimensional vector of outputs in the **training set**, the linear model will predict the $\\mathbf{y}$ given $\\mathbf{\\mathbf{X}}$ using the **parameter vector**, or **weight vector** $\\mathbf{\\boldsymbol{\\beta}} \\in \\mathrm{R}^P$ according to\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{\\mathbf{X}} \\boldsymbol{\\boldsymbol{\\beta}} + \\boldsymbol{\\varepsilon},\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\varepsilon} \\in \\mathrm{R}^N$ are the **residuals**, or the errors of the prediction. The $\\boldsymbol{\\beta}$ is found by minimizing an **objective function**, which is the **loss function**, $\\mathcal{L}(\\boldsymbol{\\beta})$, i.e. the error measured on the data. This error is the **sum of squared errors (SSE) loss**. Minimizing the SSE is the Ordinary Least Square **OLS** regression as objective function.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{OLS}(\\boldsymbol{\\beta}) &= \\mathcal{L}(\\boldsymbol{\\beta})\\\\\n",
    "               &= \\text{SSE}(\\boldsymbol{\\beta})\\\\\n",
    "               &= \\sum_i^N (y_i - \\mathbf{x}_i^T\\boldsymbol{\\beta})^2\\\\\n",
    "               &= (\\mathbf{y} - \\mathbf{\\mathbf{X}}\\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\\\\n",
    "               &= \\|\\mathbf{y} - \\mathbf{\\mathbf{X}}\\boldsymbol{\\beta}\\|_2^2,\n",
    "\\end{align}\n",
    "\n",
    "which is a simple **ordinary least squares (OLS)** minimization.\n",
    "\n",
    "## Linear regression with scikit-learn\n",
    "\n",
    "Scikit learn offer many models for supervised learning, and they all follow the same application programming interface (API), namely:\n",
    "```\n",
    "model = Estimator()\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as metrics\n",
    "%matplotlib inline\n",
    "\n",
    "# Fit Ordinary Least Squares: OLS\n",
    "csv = pd.read_csv('ftp://ftp.cea.fr/pub/unati/people/educhesnay/pystatml/data/Advertising.csv', index_col=0)\n",
    "X = csv[['TV', 'Radio']]\n",
    "y = csv['Sales']\n",
    "\n",
    "lr = lm.LinearRegression().fit(X, y)\n",
    "y_pred = lr.predict(X)\n",
    "print(\"R-squared =\", metrics.r2_score(y, y_pred))\n",
    "\n",
    "print(\"Coefficients =\", lr.coef_)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(csv['TV'], csv['Radio'], csv['Sales'], c='r', marker='o')\n",
    "\n",
    "xx1, xx2 = np.meshgrid(\n",
    "    np.linspace(csv['TV'].min(), csv['TV'].max(), num=10),\n",
    "    np.linspace(csv['Radio'].min(), csv['Radio'].max(), num=10))\n",
    "\n",
    "\n",
    "XX = np.column_stack([xx1.ravel(), xx2.ravel()])\n",
    "\n",
    "yy = lr.predict(XX)\n",
    "ax.plot_surface(xx1, xx2, yy.reshape(xx1.shape), color='None')\n",
    "ax.set_xlabel('TV')\n",
    "ax.set_ylabel('Radio')\n",
    "_ = ax.set_zlabel('Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "In statistics and machine learning, overfitting occurs when a statistical model describes random errors or noise instead of the underlying relationships. Overfitting generally occurs when a model is **excessively complex**, such as having **too many parameters relative to the number of observations**. A model that has been overfit will generally have poor predictive performance, as it can exaggerate minor fluctuations in the data.\n",
    "\n",
    "A learning algorithm is trained using some set of training samples. If the learning algorithm has the capacity to overfit the training samples the performance on the **training sample set** will improve while the performance on unseen **test sample set** will decline.\n",
    "\n",
    "The overfitting phenomenon has three main explanations:\n",
    " - excessively complex models,\n",
    " - multicollinearity, and\n",
    " - high dimensionality.\n",
    "\n",
    "### Model complexity\n",
    "\n",
    "Complex learners with too many parameters relative to the number of observations may overfit the training dataset.\n",
    "\n",
    "\n",
    "### Multicollinearity\n",
    "\n",
    "Predictors are highly correlated, meaning that one can be linearly predicted from the others. In this situation the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least not within the sample data set; it only affects computations regarding individual predictors. That is, a multiple regression model with correlated predictors can indicate how well the entire bundle of predictors predicts the outcome variable, but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others. In case of perfect multicollinearity the predictor matrix is singular and therefore cannot be inverted. Under these circumstances, for a general linear model $\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, the ordinary least-squares estimator, $\\boldsymbol{\\beta}_{OLS} = (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{y}$, does not exist.\n",
    "\n",
    "An example where correlated predictor may produce an unstable model follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bv = np.array([10, 20, 30, 40, 50])             # business volume\n",
    "tax  = .2 * bv                                  # Tax\n",
    "bp = .1 * bv + np.array([-.1, .2, .1, -.2, .1]) # business potential\n",
    "\n",
    "X = np.column_stack([bv, tax])\n",
    "beta_star = np.array([.1, 0])  # true solution\n",
    "\n",
    "'''\n",
    "Since tax and bv are correlated, there is an infinite number of linear combinations\n",
    "leading to the same prediction.\n",
    "'''\n",
    " \n",
    "# 10 times the bv then subtract it 9 times using the tax variable: \n",
    "beta_medium = np.array([.1 * 10, -.1 * 9 * (1/.2)])\n",
    "# 100 times the bv then subtract it 99 times using the tax variable: \n",
    "beta_large = np.array([.1 * 100, -.1 * 99 * (1/.2)])\n",
    "\n",
    "# Check that all model lead to the same result\n",
    "assert np.all(np.dot(X, beta_star) == np.dot(X, beta_medium))\n",
    "assert np.all(np.dot(X, beta_star) == np.dot(X, beta_large))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity between the predictors:  business volumes and tax produces unstable models with arbitrary large coefficients.\n",
    "![Multicollinearity between the predictors](images/shrinkage/ols_multicollinearity.png)\n",
    "\n",
    "Dealing with multicollinearity:\n",
    "\n",
    "- Regularisation by e.g. $\\ell_2$ shrinkage: Introduce a bias in the solution by making $(X^T X)^{-1}$ non-singular. See $\\ell_2$ shrinkage.\n",
    "\n",
    "- Feature selection: select a small number of features. See: Isabelle Guyon and André Elisseeff *An introduction to variable and feature selection* The Journal of Machine Learning Research, 2003.\n",
    "\n",
    "- Feature selection: select a small number of features using $\\ell_1$ shrinkage.\n",
    "\n",
    "- Extract few independent (uncorrelated) features using e.g. principal components analysis (PCA), partial least squares regression (PLS-R) or regression methods that cut the number of predictors to a smaller set of uncorrelated components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High dimensionality\n",
    "\n",
    "High dimensions means a large number of input features. Linear predictor associate one parameter to each input feature, so a high-dimensional situation ($P$, number of features, is large) with a relatively small number of samples $N$ (so-called large $P$ small $N$ situation) generally lead to an overfit of the training data. Thus it is generally a bad idea to add many input features into the learner. This phenomenon is called the **curse of dimensionality**.\n",
    "\n",
    "One of the most important criteria to use when choosing a learning algorithm is based on the relative size of $P$ and $N$.\n",
    "\n",
    "- Remenber that the \"covariance\" matrix $\\mathbf{X}^T\\mathbf{X}$ used in the linear model is a $P \\times P$ matrix of rank $\\min(N, P)$. Thus if $P > N$ the equation system is overparameterized and admit an infinity of solutions that might be specific to the learning dataset. See also ill-conditioned or singular matrices.\n",
    "\n",
    "- The sampling density of $N$ samples in an $P$-dimensional space is proportional to $N^{1/P}$. Thus a high-dimensional space becomes very sparse, leading to poor estimations of samples densities.\n",
    "\n",
    "- Another consequence of the sparse sampling in high dimensions is that all sample points are close to an edge of the sample. Consider $N$ data points uniformly distributed in a $P$-dimensional unit ball centered at the origin. Suppose we consider a nearest-neighbor estimate at the origin. The median distance from the origin to the closest data point is given by the expression\n",
    "\n",
    "$$\n",
    "d(P, N) = \\left(1 - \\frac{1}{2}^N\\right)^{1/P}.\n",
    "$$\n",
    "\n",
    "A more complicated expression exists for the mean distance to the closest point. For N = 500, P = 10 , $d(P, N ) \\approx 0.52$, more than halfway to the boundary. Hence most data points are closer to the boundary of the sample space than to any other data point. The reason that this presents a problem is that prediction is much more difficult near the edges of the training sample. One must extrapolate from neighboring sample points rather than interpolate between them.\n",
    "(Source: T Hastie, R Tibshirani, J Friedman. *The Elements of Statistical Learning: Data Mining, Inference, and Prediction.* Second Edition, 2009.)\n",
    "\n",
    "- Structural risk minimization provides a theoretical background of this phenomenon. (See VC dimension.)\n",
    "\n",
    "- See also bias–variance trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn # nicer plots\n",
    "\n",
    "def fit_on_increasing_size(model):\n",
    "    n_samples = 100\n",
    "    n_features_ = np.arange(10, 800, 20)\n",
    "    r2_train, r2_test, snr = [], [], []\n",
    "    for n_features in n_features_:\n",
    "        # Sample the dataset (* 2 nb of samples)\n",
    "        n_features_info = int(n_features/10)\n",
    "        np.random.seed(42)  # Make reproducible\n",
    "        X = np.random.randn(n_samples * 2, n_features)\n",
    "        beta = np.zeros(n_features)\n",
    "        beta[:n_features_info] = 1\n",
    "        Xbeta = np.dot(X, beta)\n",
    "        eps = np.random.randn(n_samples * 2)\n",
    "        y =  Xbeta + eps\n",
    "        # Split the dataset into train and test sample\n",
    "        Xtrain, Xtest = X[:n_samples, :], X[n_samples:, :]\n",
    "        ytrain, ytest = y[:n_samples], y[n_samples:]\n",
    "        # fit/predict\n",
    "        lr = model.fit(Xtrain, ytrain)\n",
    "        y_pred_train = lr.predict(Xtrain)\n",
    "        y_pred_test = lr.predict(Xtest)\n",
    "        snr.append(Xbeta.std() / eps.std())\n",
    "        r2_train.append(metrics.r2_score(ytrain, y_pred_train))\n",
    "        r2_test.append(metrics.r2_score(ytest, y_pred_test))\n",
    "    return n_features_, np.array(r2_train), np.array(r2_test), np.array(snr)\n",
    "\n",
    "def plot_r2_snr(n_features_, r2_train, r2_test, xvline, snr, ax):\n",
    "    \"\"\"\n",
    "    Two scales plot. Left y-axis: train test r-squared. Right y-axis SNR.\n",
    "    \"\"\"\n",
    "    ax.plot(n_features_, r2_train, label=\"Train r-squared\", linewidth=2)\n",
    "    ax.plot(n_features_, r2_test, label=\"Test r-squared\", linewidth=2)\n",
    "    ax.axvline(x=xvline, linewidth=2, color='k', ls='--')\n",
    "    ax.axhline(y=0, linewidth=1, color='k', ls='--')\n",
    "    ax.set_ylim(-0.2, 1.1)\n",
    "    ax.set_xlabel(\"Number of input features\")\n",
    "    ax.set_ylabel(\"r-squared\")\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_title(\"Prediction perf.\")\n",
    "    ax_right = ax.twinx()\n",
    "    ax_right.plot(n_features_, snr, 'r-', label=\"SNR\", linewidth=1)\n",
    "    ax_right.set_ylabel(\"SNR\", color='r')\n",
    "    for tl in ax_right.get_yticklabels():\n",
    "        tl.set_color('r')\n",
    "\n",
    "# Model = linear regression\n",
    "mod = lm.LinearRegression()\n",
    "\n",
    "# Fit models on dataset\n",
    "n_features, r2_train, r2_test, snr = fit_on_increasing_size(model=mod)\n",
    "\n",
    "argmax = n_features[np.argmax(r2_test)]\n",
    "\n",
    "# plot\n",
    "fig, axis = plt.subplots(1, 2, figsize=(9, 3))\n",
    "\n",
    "# Left pane: all features\n",
    "plot_r2_snr(n_features, r2_train, r2_test, argmax, snr, axis[0])\n",
    "\n",
    "# Right pane: Zoom on 100 first features\n",
    "plot_r2_snr(n_features[n_features <= 100], \n",
    "            r2_train[n_features <= 100], r2_test[n_features <= 100],\n",
    "            argmax,\n",
    "            snr[n_features <= 100],\n",
    "            axis[1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Study the code above and:\n",
    "\n",
    "- Describe the datasets: $N$: `nb_samples`, $P$: `nb_features`.\n",
    "\n",
    "- What is `n_features_info`?\n",
    "\n",
    "- Give the equation of the generative model.\n",
    "\n",
    "- What is modified by the loop?\n",
    "\n",
    "- What is the SNR?\n",
    "\n",
    "\n",
    "Comment the graph above, in terms of training and test performances:\n",
    "\n",
    "- How does the train and test performance changes as a function of $x$?\n",
    "\n",
    "- Is it the expected results when compared to the SNR?\n",
    "\n",
    "- What can you conclude?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression ($\\ell_2$-regularization)\n",
    "\n",
    "Overfitting generally leads to excessively complex weight vectors, accounting for noise or spurious correlations within predictors. To avoid this phenomenon the learning should **constrain the solution** in order to fit a global pattern. This constraint will reduce (bias) the capacity of the learning algorithm. Adding such a penalty will force the coefficients to be small, i.e. to shrink them toward zeros.\n",
    "\n",
    "Therefore the **loss function** $\\mathcal{L}(\\boldsymbol{\\beta})$ (generally the SSE) is combined with a **penalty function** $\\Omega(\\boldsymbol{\\beta})$ leading to the general form:\n",
    "\n",
    "$$\n",
    "\\text{Penalized}(\\boldsymbol{\\beta}) = \\mathcal{L}(\\boldsymbol{\\beta}) + \\lambda \\Omega(\\boldsymbol{\\beta})\n",
    "$$\n",
    "\n",
    "The respective contribution of the loss and the penalty is controlled by the **regularization parameter** $\\lambda$.\n",
    "\n",
    "Ridge regression impose a $\\ell_2$ penalty on the coefficients, i.e. it penalizes with the Euclidean norm of the coefficients while minimizing SSE. The objective function becomes:\n",
    "\n",
    "$$\n",
    "\\text{Ridge}(\\boldsymbol{\\beta}) = \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2.\n",
    "$$\n",
    "\n",
    "The $\\boldsymbol{\\beta}$ that minimises $F_{Ridge}(\\boldsymbol{\\beta})$ can be found by the following derivation:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_{\\boldsymbol{\\beta}}\\text{Ridge}(\\boldsymbol{\\beta}) &= 0\\\\\n",
    "\\nabla_{\\boldsymbol{\\beta}}\\big((\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) + \\lambda \\boldsymbol{\\beta}^T\\boldsymbol{\\beta}\\big) &= 0\\\\\n",
    "\\nabla_{\\boldsymbol{\\beta}}\\big((\\mathbf{y}^T\\mathbf{y} - 2 \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{y} + \\boldsymbol{\\beta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} + \\lambda \\boldsymbol{\\beta}^T\\boldsymbol{\\beta})\\big) &= 0\\\\\n",
    "-2\\mathbf{X}^T\\mathbf{y} + 2 \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} + 2 \\lambda \\boldsymbol{\\beta} &= 0\\\\\n",
    "-\\mathbf{X}^T\\mathbf{y} + (\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I}) \\boldsymbol{\\beta} &= 0\\\\\n",
    "(\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I}) \\boldsymbol{\\beta} &= \\mathbf{X}^T\\mathbf{y}\\\\\n",
    "\\boldsymbol{\\beta} &= (\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "- The solution adds a positive constant to the diagonal of $\\mathbf{X}^T\\mathbf{X}$ before inversion. This makes the problem nonsingular, even if $\\mathbf{X}^T\\mathbf{X}$ is not of full rank, and was the main motivation behind ridge regression.\n",
    "\n",
    "- Increasing $\\lambda$ shrinks the $\\boldsymbol{\\beta}$ coefficients toward 0.\n",
    "\n",
    "- This approach **penalizes** the objective function by the **Euclidian ($\\ell_2$) norm** of the coefficients such that solutions with large coefficients become unattractive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ridge penalty shrinks the coefficients toward zero. The figure illustrates: the OLS solution on the left. The $\\ell_1$ and $\\ell_2$ penalties in the middle pane. The penalized OLS in the right pane. The right pane shows how the penalties shrink the coefficients toward zero. The black points are the minimum found in each case, and the white points represents the true solution used to generate the data.\n",
    "\n",
    "![$\\ell_1$ and $\\ell_2$ shrinkages](images/shrinkage/ols_l1_l2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "\n",
    "# lambda is alpha!\n",
    "mod = lm.Ridge(alpha=10)\n",
    "\n",
    "# Fit models on dataset\n",
    "n_features, r2_train, r2_test, snr = fit_on_increasing_size(model=mod)\n",
    "\n",
    "argmax = n_features[np.argmax(r2_test)]\n",
    "\n",
    "# plot\n",
    "fig, axis = plt.subplots(1, 2, figsize=(9, 3))\n",
    "\n",
    "# Left pane: all features\n",
    "plot_r2_snr(n_features, r2_train, r2_test, argmax, snr, axis[0])\n",
    "\n",
    "# Right pane: Zoom on 100 first features\n",
    "plot_r2_snr(n_features[n_features <= 100], \n",
    "            r2_train[n_features <= 100], r2_test[n_features <= 100],\n",
    "            argmax,\n",
    "            snr[n_features <= 100],\n",
    "            axis[1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice\n",
    "\n",
    "What benefit has been obtained by using $\\ell_2$ regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regression ($\\ell_1$-regularization)\n",
    "\n",
    "Lasso regression penalizes the coefficients by the $\\ell_1$ norm. This constraint will reduce (bias) the capacity of the learning algorithm. To add such a penalty forces the coefficients to be small, i.e. it shrinks them toward zero. The objective function to minimize becomes:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Lasso}(\\boldsymbol{\\beta}) &= \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda\\|\\boldsymbol{\\beta}\\|_1.\n",
    "\\end{align}\n",
    "\n",
    "This penalty forces some coefficients to be exactly zero, providing a feature selection property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "\n",
    "# lambda is alpha !\n",
    "mod = lm.Lasso(alpha=.1)\n",
    "\n",
    "# Fit models on dataset\n",
    "n_features, r2_train, r2_test, snr = fit_on_increasing_size(model=mod)\n",
    "\n",
    "argmax = n_features[np.argmax(r2_test)]\n",
    "\n",
    "# plot\n",
    "fig, axis = plt.subplots(1, 2, figsize=(9, 3))\n",
    "\n",
    "# Left pane: all features\n",
    "plot_r2_snr(n_features, r2_train, r2_test, argmax, snr, axis[0])\n",
    "\n",
    "# Right pane: Zoom on 200 first features\n",
    "plot_r2_snr(n_features[n_features <= 200], \n",
    "            r2_train[n_features <= 200], r2_test[n_features <= 200],\n",
    "            argmax,\n",
    "            snr[n_features <= 200],\n",
    "            axis[1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity of the $\\ell_1$ norm\n",
    "\n",
    "#### Occam's razor\n",
    "\n",
    "Occam's razor (also written as Ockham's razor, and **lex parsimoniae** in Latin, which means law of parsimony) is a problem solving principle attributed to William of Ockham (1287-1347), who was an English Franciscan friar and scholastic philosopher and theologian. The principle can be interpreted as stating that **among competing hypotheses, the one with the fewest assumptions should be selected**.\n",
    "\n",
    "#### Principle of parsimony\n",
    "\n",
    "The simplest of two competing theories is to be preferred. Definition of parsimony: Economy of explanation in conformity with Occam's razor.\n",
    "\n",
    "Among possible models with similar loss, choose the simplest one: \n",
    "\n",
    "- Choose the model with the smallest coefficient vector, i.e. smallest $\\ell_2$ ($\\|\\boldsymbol{\\beta}\\|_2$) or $\\ell_1$ ($\\|\\boldsymbol{\\beta}\\|_1$) norm of $\\boldsymbol{\\beta}$, i.e. $\\ell_2$ or $\\ell_1$ penalty. See also bias-variance tradeoff.\n",
    "\n",
    "- Choose the model that uses the smallest number of predictors. In other words, choose the model that has many predictors with zero weights. Two approaches are available to obtain this: (i) Perform a feature selection as a preprocessing prior to applying the learning algorithm, or (ii) embed the feature selection procedure within the learning process.\n",
    "\n",
    "#### Sparsity-induced penalty or embedded feature selection with the $\\ell_1$ penalty\n",
    "\n",
    "The penalty based on the $\\ell_1$ norm promotes **sparsity** (scattered, or not dense): it forces many coefficients to be exactly zero. This also makes the coefficient vector scattered.\n",
    "\n",
    "The figure bellow illustrates the OLS loss under a constraint acting on the $\\ell_1$ norm of the coefficient vector. I.e., it illustrates the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\underset{\\boldsymbol{\\beta}}{\\text{minimize}} ~& \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 \\\\\n",
    "    \\text{subject to}                 ~& \\|\\boldsymbol{\\beta}\\|_1 \\leq 1.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "![Sparsity of L1 norm](images/shrinkage/l1_sparse.png)\n",
    "\n",
    "### Optimization issues\n",
    "\n",
    "*Section to be completed*\n",
    "\n",
    "- No more closed-form solution.\n",
    "\n",
    "- Convex but not differentiable.\n",
    "\n",
    "- Requires specific optimization algorithms, such as the fast iterative shrinkage-thresholding algorithm (FISTA): Amir Beck and Marc Teboulle, *A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems* SIAM J. Imaging Sci., 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic-net regression ($\\ell_2$-$\\ell_1$-regularization)\n",
    "\n",
    "The Elastic-net estimator combines the $\\ell_1$ and $\\ell_2$ penalties, and results in the problem to\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Enet}(\\boldsymbol{\\beta}) &= \\|\\mathbf{y} - \\mathbf{X}^T\\boldsymbol{\\beta}\\|_2^2 + \\alpha \\left(\\rho~\\|\\boldsymbol{\\beta}\\|_1 + (1-\\rho)~\\|\\boldsymbol{\\beta}\\|_2^2 \\right),\n",
    "\\end{align}\n",
    "\n",
    "where $\\alpha$ acts as a global penalty and $\\rho$ as an $\\ell_1 / \\ell_2$ ratio.\n",
    "\n",
    "### Rationale\n",
    "\n",
    "- If there are groups of highly correlated variables, Lasso tends to arbitrarily select only one from each group. These models are difficult to interpret because covariates that are strongly associated with the outcome are not included in the predictive model. Conversely, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.\n",
    "\n",
    "- Studies on real world data and simulation studies show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "\n",
    "mod = lm.ElasticNet(alpha=.5, l1_ratio=.5)\n",
    "\n",
    "# Fit models on dataset\n",
    "n_features, r2_train, r2_test, snr = fit_on_increasing_size(model=mod)\n",
    "\n",
    "argmax = n_features[np.argmax(r2_test)]\n",
    "\n",
    "# plot\n",
    "fig, axis = plt.subplots(1, 2, figsize=(9, 3))\n",
    "\n",
    "# Left pane: all features\n",
    "plot_r2_snr(n_features, r2_train, r2_test, argmax, snr, axis[0])\n",
    "\n",
    "# Right pane: Zoom on 100 first features\n",
    "plot_r2_snr(n_features[n_features <= 100], \n",
    "            r2_train[n_features <= 100], r2_test[n_features <= 100],\n",
    "            argmax,\n",
    "            snr[n_features <= 100],\n",
    "            axis[1])\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
