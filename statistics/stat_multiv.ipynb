{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate statistics\n",
    "\n",
    "Multivariate statistics includes all statistical techniques for analyzing samples made of two or more variables.\n",
    "The data set (a $N \\times P$ matrix $\\mathbf{X}$) is a collection of $N$ independent samples column **vectors** $[\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{i}, \\ldots, \\mathbf{x}_{N}]$ of length $P$\n",
    "\n",
    "$$\n",
    "\\mathbf{X} =\n",
    "    \\begin{bmatrix}\n",
    "        -\\mathbf{x}_{1}^T- \\\\\n",
    "        \\vdots \\\\\n",
    "        -\\mathbf{x}_{i}^T- \\\\\n",
    "        \\vdots \\\\\n",
    "        -\\mathbf{x}_{P}^T-\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "        x_{11} & \\cdots & x_{1j} & \\cdots & x_{1P} \\\\\n",
    "        \\vdots &        & \\vdots &        & \\vdots \\\\\n",
    "        x_{i1} & \\cdots & x_{ij} & \\cdots & x_{iP} \\\\\n",
    "        \\vdots &        & \\vdots &        & \\vdots \\\\\n",
    "        x_{N1} & \\cdots & x_{Nj} & \\cdots & x_{NP}\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "        x_{11} & \\ldots     & x_{1P} \\\\\n",
    "        \\vdots &            & \\vdots \\\\\n",
    "               & \\mathbf{X} & \\\\\n",
    "        \\vdots &            & \\vdots \\\\\n",
    "        x_{N1} & \\ldots     & x_{NP}\n",
    "    \\end{bmatrix}_{N \\times P}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra\n",
    "\n",
    "### Euclidean norm and distance\n",
    "\n",
    "The Euclidean norm of a vector $\\mathbf{a} \\in \\mathbb{R}^P$ is denoted\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{a}\\|_2 = \\sqrt{\\sum_i^P {a_i}^2}\n",
    "$$\n",
    "\n",
    "The Euclidean distance between two vectors $\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^P$ is\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{a}-\\mathbf{b}\\|_2 = \\sqrt{\\sum_i^P (a_i-b_i)^2}\n",
    "$$\n",
    "\n",
    "### Dot product and projection\n",
    "\n",
    "Source: [Wikipedia](https://en.wikipedia.org/wiki/Projection_%28linear_algebra%29)\n",
    "\n",
    "**Algebraic definition**\n",
    "\n",
    "The dot product, denoted ''$\\cdot$'' of two $P$-dimensional vectors $\\mathbf{a} = [a_1, a_2, ..., a_P]$ and $\\mathbf{a} = [b_1, b_2, ..., b_P]$ is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{a}^T \\mathbf{b} = \\sum_i a_i b_i =\n",
    "    \\begin{bmatrix}\n",
    "        a_{1} & \\ldots &  \\mathbf{a}^T  & \\ldots & a_{P}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        b_{1}\\\\\n",
    "        \\vdots \\\\\n",
    "        \\mathbf{b}\\\\ \n",
    "        \\vdots\\\\\n",
    "        b_{P}\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The Euclidean norm of a vector can be computed using the dot product, as\n",
    "\n",
    "$$\n",
    "\\left\\|\\mathbf{a} \\right\\|_2 = {\\sqrt {\\mathbf{a} \\cdot \\mathbf{a}}}.\n",
    "$$\n",
    "\n",
    "**Geometric definition: projection**\n",
    "\n",
    "In Euclidean space, a Euclidean vector is a geometrical object that possesses both a magnitude and a direction. A vector can be pictured as an arrow. Its magnitude is its length, and its direction is the direction that the arrow points. The magnitude of a vector $\\mathbf{a}$ is denoted by $\\|\\mathbf{a}\\|_2$. The dot product of two Euclidean vectors $\\mathbf{a}$ and $\\mathbf{b}$ is defined by\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a} \\|_2\\ \\|\\mathbf{b} \\|_2\\cos \\theta,\n",
    "$$\n",
    "\n",
    "where $\\theta$ is the angle between $\\mathbf{a}$ and $\\mathbf{b}$.\n",
    "\n",
    "In particular, if $\\mathbf{a}$ and $\\mathbf{b}$ are orthogonal, then the angle between them is 90° and\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = 0.\n",
    "$$\n",
    "\n",
    "At the other extreme, if they are codirectional, then the angle between them is 0° and\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = \\left\\|\\mathbf{a} \\right\\|_2\\,\\left\\|\\mathbf{b} \\right\\|_2\n",
    "$$\n",
    "\n",
    "This implies that the dot product of a vector $\\mathbf{a}$ by itself is\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{a} = \\left\\|\\mathbf{a} \\right\\|_2^2.\n",
    "$$\n",
    "\n",
    "The scalar projection (or scalar component) of a Euclidean vector $\\mathbf{a}$ in the direction of a Euclidean vector $\\mathbf{b}$ is given by\n",
    "\n",
    "$$\n",
    "a_{b} = \\left\\|\\mathbf{a} \\right\\|_2\\cos \\theta,\n",
    "$$\n",
    "\n",
    "where $\\theta$ is the angle between $\\mathbf{a}$ and $\\mathbf{b}$.\n",
    "\n",
    "In terms of the geometric definition of the dot product, this can be rewritten\n",
    "\n",
    "$$\n",
    "a_{b} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{b}\\|_2},\n",
    "$$\n",
    "\n",
    "![Projection.](images/Dot_Product.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "a = np.random.randn(10)\n",
    "b = np.random.randn(10)\n",
    "\n",
    "np.dot(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean vector\n",
    "\n",
    "The mean ($P \\times 1$) column-vector $\\mathbf{\\mu}$ whose estimator is\n",
    "$$\n",
    "\\bar{\\mathbf{x}} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{x_i} =\n",
    "    \\frac{1}{N}\\sum_{i=1}^N\n",
    "        \\begin{bmatrix}\n",
    "            x_{i1}\\\\\n",
    "            \\vdots\\\\\n",
    "            x_{ij}\\\\\n",
    "            \\vdots\\\\\n",
    "            x_{iP}\\\\\n",
    "         \\end{bmatrix} =\n",
    "         \\begin{bmatrix}\n",
    "             \\bar{x}_{1}\\\\\n",
    "             \\vdots\\\\\n",
    "             \\bar{x}_{j}\\\\\n",
    "             \\vdots\\\\\n",
    "             \\bar{x}_{P}\\\\\n",
    "         \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "## Covariance matrix\n",
    "\n",
    "- The covariance matrix $\\mathbf{\\Sigma_{XX}}$ is a **symmetric** positive semi-definite matrix whose element in the $j, k$ position is the covariance between the $j^{th}$ and $k^{th}$ elements of a random vector i.e. the $j^{th}$ and $k^{th}$ columns of $\\mathbf{X}$.\n",
    "\n",
    "- The covariance matrix generalizes the notion of covariance to multiple dimensions.\n",
    "\n",
    "- The covariance matrix describe the shape of the sample distribution around the mean assuming an elliptical distribution:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Sigma_{XX}} = E(\\mathbf{X}-E(\\mathbf{X}))^TE(\\mathbf{X}-E(\\mathbf{X})),\n",
    "$$\n",
    "\n",
    "whose estimator $\\mathbf{S_{XX}}$ is a $P \\times P$ matrix given by\n",
    "\n",
    "$$\n",
    "\\mathbf{S_{XX}}= \\frac{1}{N-1}(\\mathbf{X}- \\mathbf{1} \\bar{\\mathbf{x}}^T)^T (\\mathbf{X}- \\mathbf{1} \\bar{\\mathbf{x}}^T).\n",
    "$$\n",
    "\n",
    "If we assume that $\\mathbf{X}$ is centered, i.e. $\\mathbf{X}$ is replaced by $\\mathbf{X} - \\mathbf{1}\\bar{\\mathbf{x}}^T$ then the estimator is\n",
    "\n",
    "$$\n",
    "\\mathbf{S_{XX}} = \\frac{1}{N-1} \\mathbf{X}^T\\mathbf{X} =\n",
    "    \\frac{1}{N-1} \\begin{bmatrix}\n",
    "                      x_{11} & \\cdots & x_{N1} \\\\\n",
    "                      x_{1j} & \\cdots & x_{Nj} \\\\\n",
    "                      \\vdots &        & \\vdots \\\\\n",
    "                      x_{1P} & \\cdots & x_{NP} \\\\\n",
    "                  \\end{bmatrix}\n",
    "                  \\begin{bmatrix}\n",
    "                      x_{11} & \\cdots & x_{1k}& x_{1P}\\\\\n",
    "                      \\vdots &        & \\vdots & \\vdots\\\\\n",
    "                      x_{N1} & \\cdots & x_{Nk}& x_{NP}\n",
    "                  \\end{bmatrix}=\n",
    "                  \\begin{bmatrix}\n",
    "                      s_{1} & \\ldots  & s_{1k} & s_{1P}\\\\\n",
    "                            & \\ddots  & s_{jk} & \\vdots\\\\\n",
    "                            &         & s_{k}  & s_{kP}\\\\      \n",
    "                            &         &        & s_{P}\\\\\n",
    "                  \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "s_{jk} = s_{kj} = \\frac{1}{N-1} \\mathbf{x_j}^T \\mathbf{x_k} = \\frac{1}{N-1} \\sum_{i=1}^N x_{ij} x_{ik}\n",
    "$$\n",
    "\n",
    "is an estimator of the covariance between the $j^{th}$ and $k^{th}$ variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Avoid warnings and force inline plot\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "##\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pystatsml.plot_utils\n",
    "import seaborn as sns  # nice color\n",
    "\n",
    "np.random.seed(42)\n",
    "colors = sns.color_palette()\n",
    "\n",
    "n_samples, n_features = 100, 2\n",
    "\n",
    "mean, Cov, X = [None] * 4, [None] * 4, [None] * 4\n",
    "mean[0] = np.array([-2.5, 2.5])\n",
    "Cov[0] = np.array([[1, 0],\n",
    "                   [0, 1]])\n",
    "\n",
    "mean[1] = np.array([2.5, 2.5])\n",
    "Cov[1] = np.array([[1, .5],\n",
    "                   [.5, 1]])\n",
    "\n",
    "mean[2] = np.array([-2.5, -2.5])\n",
    "Cov[2] = np.array([[1, .9],\n",
    "                   [.9, 1]])\n",
    "\n",
    "mean[3] = np.array([2.5, -2.5])\n",
    "Cov[3] = np.array([[1, -.9],\n",
    "                   [-.9, 1]])\n",
    "\n",
    "# Generate dataset\n",
    "for i in range(len(mean)):\n",
    "    X[i] = np.random.multivariate_normal(mean[i], Cov[i], n_samples)\n",
    "\n",
    "# Plot\n",
    "for i in range(len(mean)):\n",
    "    # Points\n",
    "    plt.scatter(X[i][:, 0], X[i][:, 1], color=colors[i], label=\"class %i\" % i)\n",
    "    # Means\n",
    "    plt.scatter(mean[i][0], mean[i][1], marker=\"o\", s=200, facecolors='w',\n",
    "                edgecolors=colors[i], linewidth=2)\n",
    "    # Ellipses representing the covariance matrices\n",
    "    pystatsml.plot_utils.plot_cov_ellipse(Cov[i], pos=mean[i], facecolor='none',\n",
    "                                          linewidth=2, edgecolor=colors[i])\n",
    "\n",
    "plt.axis('equal')\n",
    "_ = plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "url = 'https://python-graph-gallery.com/wp-content/uploads/mtcars.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "f, ax = plt.subplots(figsize=(5.5, 4.5))\n",
    "cmap = sns.color_palette(\"RdBu_r\", 11)\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "_ = sns.heatmap(corr, mask=None, cmap=cmap, vmax=1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-order correlation matrix using AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert correlation to distances\n",
    "d = 2 * (1 - np.abs(corr))\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering = AgglomerativeClustering(n_clusters=3, linkage='single', affinity=\"precomputed\").fit(d)\n",
    "lab=0\n",
    "\n",
    "clusters = [list(corr.columns[clustering.labels_==lab]) for lab in set(clustering.labels_)]\n",
    "print(clusters)\n",
    "\n",
    "reordered = np.concatenate(clusters)\n",
    "\n",
    "R = corr.loc[reordered, reordered]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(5.5, 4.5))\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "_ = sns.heatmap(R, mask=None, cmap=cmap, vmax=1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision matrix\n",
    "\n",
    "In statistics, precision is the reciprocal of the variance, and the precision matrix is the matrix inverse of the covariance matrix.\n",
    "\n",
    "It is related to **partial correlations** that measures the degree of association between two variables, while controlling the effect of other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Cov = np.array([[1.0, 0.9, 0.9, 0.0, 0.0, 0.0],\n",
    "                [0.9, 1.0, 0.9, 0.0, 0.0, 0.0],\n",
    "                [0.9, 0.9, 1.0, 0.0, 0.0, 0.0],\n",
    "                [0.0, 0.0, 0.0, 1.0, 0.9, 0.0],\n",
    "                [0.0, 0.0, 0.0, 0.9, 1.0, 0.0],\n",
    "                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "print(\"# Precision matrix:\")\n",
    "Prec = np.linalg.inv(Cov)\n",
    "print(Prec.round(2))\n",
    "\n",
    "print(\"# Partial correlations:\")\n",
    "Pcor = np.zeros(Prec.shape)\n",
    "Pcor[::] = np.NaN\n",
    "\n",
    "for i, j in zip(*np.triu_indices_from(Prec, 1)):\n",
    "    Pcor[i, j] = - Prec[i, j] / np.sqrt(Prec[i, i] * Prec[j, j])\n",
    "\n",
    "print(Pcor.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mahalanobis distance\n",
    "\n",
    "- The Mahalanobis distance is a measure of the distance between two points $\\mathbf{x}$ and $\\mathbf{\\mu}$ where the dispersion (i.e. the covariance structure) of the samples is taken into account.\n",
    "\n",
    "- The dispersion is considered through covariance matrix.\n",
    "\n",
    "This is formally expressed as\n",
    "\n",
    "$$\n",
    "D_M(\\mathbf{x}, \\mathbf{\\mu}) = \\sqrt{(\\mathbf{x} - \\mathbf{\\mu})^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu})}.\n",
    "$$\n",
    "\n",
    "**Intuitions**\n",
    "\n",
    "- Distances along the principal directions of dispersion are contracted since they correspond to likely dispersion of points.\n",
    "\n",
    "- Distances othogonal to the principal directions of dispersion are dilated since they correspond to unlikely dispersion of points. \n",
    "\n",
    "\n",
    "For example\n",
    "$$\n",
    "D_M(\\mathbf{1}) = \\sqrt{\\mathbf{1}^T \\mathbf{\\Sigma}^{-1}\\mathbf{1}}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones  = np.ones(Cov.shape[0])\n",
    "d_euc = np.sqrt(np.dot(ones, ones))\n",
    "d_mah = np.sqrt(np.dot(np.dot(ones, Prec), ones))\n",
    "\n",
    "print(\"Euclidean norm of ones=%.2f. Mahalanobis norm of ones=%.2f\" % (d_euc, d_mah))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dot product that distances along the principal directions of dispersion are contracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.dot(ones, Prec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pystatsml.plot_utils\n",
    "%matplotlib inline\n",
    "np.random.seed(40)\n",
    "colors = sns.color_palette()\n",
    "\n",
    "mean = np.array([0, 0]) \n",
    "Cov = np.array([[1, .8],\n",
    "                [.8, 1]])\n",
    "samples = np.random.multivariate_normal(mean, Cov, 100)\n",
    "x1 = np.array([0, 2])\n",
    "x2 = np.array([2, 2])\n",
    "\n",
    "plt.scatter(samples[:, 0], samples[:, 1], color=colors[0])\n",
    "plt.scatter(mean[0], mean[1], color=colors[0], s=200, label=\"mean\")\n",
    "plt.scatter(x1[0], x1[1], color=colors[1], s=200, label=\"x1\")\n",
    "plt.scatter(x2[0], x2[1], color=colors[2], s=200, label=\"x2\")\n",
    "\n",
    "# plot covariance ellipsis\n",
    "pystatsml.plot_utils.plot_cov_ellipse(Cov, pos=mean, facecolor='none', \n",
    "                                      linewidth=2, edgecolor=colors[0])\n",
    "# Compute distances\n",
    "d2_m_x1 = scipy.spatial.distance.euclidean(mean, x1)\n",
    "d2_m_x2 = scipy.spatial.distance.euclidean(mean, x2)\n",
    "\n",
    "Covi = scipy.linalg.inv(Cov)\n",
    "dm_m_x1 = scipy.spatial.distance.mahalanobis(mean, x1, Covi)\n",
    "dm_m_x2 = scipy.spatial.distance.mahalanobis(mean, x2, Covi)\n",
    "\n",
    "# Plot distances\n",
    "vm_x1 = (x1 - mean) / d2_m_x1\n",
    "vm_x2 = (x2 - mean) / d2_m_x2\n",
    "jitter = .1\n",
    "plt.plot([mean[0] - jitter, d2_m_x1 * vm_x1[0] - jitter],\n",
    "         [mean[1], d2_m_x1 * vm_x1[1]], color='k')\n",
    "plt.plot([mean[0] - jitter, d2_m_x2 * vm_x2[0] - jitter],\n",
    "         [mean[1], d2_m_x2 * vm_x2[1]], color='k')\n",
    "\n",
    "plt.plot([mean[0] + jitter, dm_m_x1 * vm_x1[0] + jitter],\n",
    "         [mean[1], dm_m_x1 * vm_x1[1]], color='r')\n",
    "plt.plot([mean[0] + jitter, dm_m_x2 * vm_x2[0] + jitter],\n",
    "         [mean[1], dm_m_x2 * vm_x2[1]], color='r')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.text(-6.1, 3,\n",
    "         'Euclidian:   d(m, x1) = %.1f<d(m, x2) = %.1f' % (d2_m_x1, d2_m_x2), color='k')\n",
    "plt.text(-6.1, 3.5,\n",
    "         'Mahalanobis: d(m, x1) = %.1f>d(m, x2) = %.1f' % (dm_m_x1, dm_m_x2), color='r')\n",
    "\n",
    "plt.axis('equal')\n",
    "print('Euclidian   d(m, x1) = %.2f < d(m, x2) = %.2f' % (d2_m_x1, d2_m_x2))\n",
    "print('Mahalanobis d(m, x1) = %.2f > d(m, x2) = %.2f' % (dm_m_x1, dm_m_x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the covariance matrix is the identity matrix, the Mahalanobis distance reduces to the Euclidean distance. If the covariance matrix is diagonal, then the resulting distance measure is called a normalized Euclidean distance.\n",
    "\n",
    "More generally, the Mahalanobis distance is a measure of the distance between a point $\\mathbf{x}$ and a distribution $\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}, \\mathbf{\\Sigma})$. It is a multi-dimensional generalization of the idea of measuring how many standard deviations away $\\mathbf{x}$ is from the mean. This distance is zero if $\\mathbf{x}$ is at the mean, and grows as $\\mathbf{x}$ moves away from the mean: along each principal component axis, it measures the number of standard deviations from $\\mathbf{x}$ to the mean of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate normal distribution\n",
    "\n",
    "The distribution, or probability density function (PDF) (sometimes just density), of a continuous random variable is a function that describes the relative likelihood for this random variable to take on a given value.\n",
    "\n",
    "The multivariate normal distribution, or multivariate Gaussian distribution, of a $P$-dimensional random vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_P]^T$ is\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}, \\mathbf{\\Sigma}) = \\frac{1}{(2\\pi)^{P/2}|\\mathbf{\\Sigma}|^{1/2}}\\exp\\{-\\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu)}^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x} - \\mathbf{\\mu})\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from scipy.stats import multivariate_normal\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def multivariate_normal_pdf(X, mean, sigma):\n",
    "    \"\"\"Multivariate normal probability density function over X (n_samples x n_features)\"\"\"\n",
    "    P = X.shape[1]\n",
    "    det = np.linalg.det(sigma)\n",
    "    norm_const = 1.0 / (((2*np.pi) ** (P/2)) * np.sqrt(det))\n",
    "    X_mu = X - mu\n",
    "    inv = np.linalg.inv(sigma)\n",
    "    d2 = np.sum(np.dot(X_mu, inv) * X_mu, axis=1)\n",
    "    return norm_const * np.exp(-0.5 * d2)\n",
    "\n",
    "# mean and covariance\n",
    "mu = np.array([0, 0])\n",
    "sigma = np.array([[1, -.5],\n",
    "                  [-.5, 1]])\n",
    "\n",
    "# x, y grid\n",
    "x, y = np.mgrid[-3:3:.1, -3:3:.1]\n",
    "X = np.stack((x.ravel(), y.ravel())).T\n",
    "norm = multivariate_normal_pdf(X, mean, sigma).reshape(x.shape)\n",
    "\n",
    "# Do it with scipy\n",
    "norm_scpy = multivariate_normal(mu, sigma).pdf(np.stack((x, y), axis=2))\n",
    "assert np.allclose(norm, norm_scpy)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(x, y, norm, rstride=3,\n",
    "        cstride=3, cmap=plt.cm.coolwarm,\n",
    "        linewidth=1, antialiased=False\n",
    "    )\n",
    "\n",
    "ax.set_zlim(0, 0.2)\n",
    "ax.zaxis.set_major_locator(plt.LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(plt.FormatStrFormatter('%.02f'))\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('p(x)')\n",
    "\n",
    "plt.title('Bivariate Normal/Gaussian distribution')\n",
    "fig.colorbar(surf, shrink=0.5, aspect=7, cmap=plt.cm.coolwarm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Dot product and Euclidean norm\n",
    "\n",
    "Given $\\mathbf{a} = [2, 1]^T$ and $\\mathbf{b} = [1, 1]^T$\n",
    "\n",
    "1. Write a function `euclidean(x)` that computes the Euclidean norm of vector, $\\mathbf{x}$.\n",
    "2. Compute the Euclidean norm of $\\mathbf{a}$.\n",
    "3. Compute the Euclidean distance of $\\|\\mathbf{a}-\\mathbf{b}\\|_2$.\n",
    "4. Compute the projection of $\\mathbf{b}$ in the direction of vector $\\mathbf{a}$: $b_{a}$.\n",
    "5. Simulate a dataset $\\mathbf{X}$ of $N=100$ samples of 2-dimensional vectors.\n",
    "6. Project all samples in the direction of the vector $\\mathbf{a}$.\n",
    "\n",
    "### Covariance matrix and Mahalanobis norm\n",
    "\n",
    "1. Sample a dataset $\\mathbf{X}$ of $N=100$ samples of 2-dimensional vectors from the bivariate normal distribution\n",
    "$\\mathcal{N}(\\mathbf{\\mu}, \\mathbf{\\Sigma})$ where $\\mathbf{\\mu}=[1, 1]^T$ and $\\mathbf{\\Sigma}=\\begin{bmatrix} 1 & 0.8\\\\0.8, 1 \\end{bmatrix}$.\n",
    "2. Compute the mean vector $\\mathbf{\\bar{x}}$ and center $\\mathbf{X}$. Compare the estimated mean $\\mathbf{\\bar{x}}$ to the true mean, $\\mathbf{\\mu}$.\n",
    "3. Compute the empirical covariance matrix $\\mathbf{S}$. Compare the estimated covariance matrix $\\mathbf{S}$ to the true covariance matrix, $\\mathbf{\\Sigma}$.\n",
    "4. Compute $\\mathbf{S}^{-1}$ (`Sinv`) the inverse of the covariance matrix by using `scipy.linalg.inv(S)`.\n",
    "5. Write a function `mahalanobis(x, xbar, Sinv)` that computes the Mahalanobis distance of a vector $\\mathbf{x}$ to the mean, $\\mathbf{\\bar{x}}$.\n",
    "6. Compute the Mahalanobis and Euclidean distances of each sample $\\mathbf{x}_i$ to the mean $\\mathbf{\\bar{x}}$. Store the results in a $100 \\times 2$ dataframe."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
