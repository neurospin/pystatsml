{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "\n",
    "## Course outline:\n",
    "\n",
    "1. Recall of linear classifier\n",
    "\n",
    "2. MLP with scikit-learn\n",
    "\n",
    "3. MLP with pytorch\n",
    "\n",
    "4. Test several MLP architectures\n",
    "\n",
    "5. Limits of MLP\n",
    "\n",
    "Sources:\n",
    "\n",
    "Deep learning\n",
    "\n",
    "- [cs231n.stanford.edu](http://cs231n.stanford.edu/)\n",
    "\n",
    "\n",
    "Pytorch\n",
    "\n",
    "- [WWW tutorials](https://pytorch.org/tutorials/)\n",
    "- [github tutorials](https://github.com/pytorch/tutorials)\n",
    "- [github examples](https://github.com/pytorch/examples)\n",
    "\n",
    "MNIST and pytorch:\n",
    "\n",
    "- [MNIST nextjournal.com/gkoehler/pytorch-mnist](https://nextjournal.com/gkoehler/pytorch-mnist)\n",
    "- [MNIST github/pytorch/examples](https://github.com/pytorch/examples/tree/master/mnist)\n",
    "- [MNIST kaggle](https://www.kaggle.com/sdelecourt/cnn-with-pytorch-for-mnist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "#\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu' # Force CPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: MNIST Handwritten Digit Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "WD = os.path.join(Path.home(), \"data\", \"pystatml\", \"dl_mnist_pytorch\")\n",
    "os.makedirs(WD, exist_ok=True)\n",
    "os.chdir(WD)\n",
    "print(\"Working dir is:\", os.getcwd())\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "def load_mnist(batch_size_train, batch_size_test):\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,)) # Mean and Std of the MNIST dataset\n",
    "                       ])),\n",
    "        batch_size=batch_size_train, shuffle=True)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)) # Mean and Std of the MNIST dataset\n",
    "        ])),\n",
    "        batch_size=batch_size_test, shuffle=True)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader = load_mnist(64, 10000)\n",
    "\n",
    "dataloaders = dict(train=train_loader, val=val_loader)\n",
    "                   \n",
    "# Info about the dataset\n",
    "D_in = np.prod(dataloaders[\"train\"].dataset.data.shape[1:])\n",
    "D_out = len(dataloaders[\"train\"].dataset.targets.unique())\n",
    "print(\"Datasets shapes:\", {x: dataloaders[x].dataset.data.shape for x in ['train', 'val']})\n",
    "print(\"N input features:\", D_in, \"Output classes:\", D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at some mini-batches examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx, (example_data, example_targets) = next(enumerate(train_loader))\n",
    "print(\"Train batch:\", example_data.shape, example_targets.shape)\n",
    "batch_idx, (example_data, example_targets) = next(enumerate(val_loader))\n",
    "print(\"Val batch:\", example_data.shape, example_targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So one test data batch is a tensor of shape: . This means we have 1000 examples of 28x28 pixels in grayscale\n",
    "(i.e. no rgb channels, hence the one). We can plot some of them using matplotlib.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data_label_prediction(data, y_true, y_pred=None, shape=(2, 3)):\n",
    "    y_pred = [None] * len(y_true) if y_pred is None else y_pred\n",
    "    fig = plt.figure()\n",
    "    for i in range(np.prod(shape)):\n",
    "        plt.subplot(*shape, i+1)\n",
    "        plt.tight_layout()\n",
    "        plt.imshow(data[i][0], cmap='gray', interpolation='none')\n",
    "        plt.title(\"True: {} Pred: {}\".format(y_true[i], y_pred[i]))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "show_data_label_prediction(data=example_data, y_true=example_targets, y_pred=None, shape=(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall of linear classifier\n",
    "\n",
    "### Binary logistic regression\n",
    "\n",
    "<img src=\"figures/logistic.png\" width=\"300\">\n",
    "\n",
    "1 neuron as output layer\n",
    "$$\n",
    "f(x) = \\sigma(x^{T} w)\n",
    "$$\n",
    "\n",
    "\n",
    "### Softmax Classifier (Multinomial Logistic Regression)\n",
    "\n",
    "<img src=\"figures/logistic_multinominal.png\" width=\"300\">\n",
    "\n",
    "- Input $x$: a vector of dimension $(0)$ (layer 0).\n",
    "- Ouput $f(x)$ a vector of $(1)$ (layer 1) possible labels \n",
    "\n",
    "The model as $(1)$ neurons as output layer\n",
    "\n",
    "$$\n",
    "f(x) = \\text{softmax}(x^{T} W + b)\n",
    "$$\n",
    "\n",
    "Where $W$ is a $(0) \\times (1)$ of coefficients and $b$ is a  $(1)$-dimentional vector of bias.\n",
    "\n",
    "MNIST classfification using multinomial logistic\n",
    "\n",
    "<img src=\"figures/logistic_multinominal_MNIST.png\" width=\"800\">\n",
    "\n",
    "[source: Logistic regression MNIST](https://notebooks.azure.com/cntk/projects/edxdle/html/Lab2_LogisticRegression.ipynb)\n",
    "\n",
    "Here we fit a multinomial logistic regression with L2 penalty on a subset of\n",
    "the MNIST digits classification task.\n",
    "\n",
    "[source: scikit-learn.org](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_loader.dataset.data.numpy()\n",
    "#print(X_train.shape)\n",
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "y_train = train_loader.dataset.targets.numpy()\n",
    "\n",
    "X_test = val_loader.dataset.data.numpy()\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "y_test = val_loader.dataset.targets.numpy()\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Turn up tolerance for faster convergence\n",
    "clf = LogisticRegression(C=50., multi_class='multinomial', solver='sag', tol=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "#sparsity = np.mean(clf.coef_ == 0) * 100\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Test score with penalty: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = clf.coef_.copy()\n",
    "plt.figure(figsize=(10, 5))\n",
    "scale = np.abs(coef).max()\n",
    "for i in range(10):\n",
    "    l1_plot = plt.subplot(2, 5, i + 1)\n",
    "    l1_plot.imshow(coef[i].reshape(28, 28), interpolation='nearest',\n",
    "                   cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)\n",
    "    l1_plot.set_xticks(())\n",
    "    l1_plot.set_yticks(())\n",
    "    l1_plot.set_xlabel('Class %i' % i)\n",
    "plt.suptitle('Classification vector for...')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Two Layer MLP\n",
    "\n",
    "### MLP with Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, ), max_iter=5, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=0.01, batch_size=64)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
    "\n",
    "print(\"Coef shape=\", len(mlp.coefs_))\n",
    "\n",
    "fig, axes = plt.subplots(4, 4)\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n",
    "for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "               vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_hidden, d_out):\n",
    "        super(TwoLayerMLP, self).__init__()\n",
    "        self.d_in = d_in\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_in, d_hidden)\n",
    "        self.linear2 = nn.Linear(d_hidden, d_out)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, self.d_in)\n",
    "        X = self.linear1(X)\n",
    "        return F.log_softmax(self.linear2(X), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model\n",
    "\n",
    "- First we want to make sure our network is in training mode.\n",
    "\n",
    "- Iterate over epochs\n",
    "\n",
    "- Alternate train and validation dataset\n",
    "\n",
    "- Iterate over all training/val data once per epoch. Loading the individual batches is handled by the DataLoader.\n",
    "\n",
    "- Set the gradients to zero using `optimizer.zero_grad()` since PyTorch by default accumulates gradients.\n",
    "\n",
    "- Forward pass:\n",
    "  * `model(inputs)`: Produce the output of our network.\n",
    "  * `torch.max(outputs, 1)`: softmax predictions.\n",
    "  * `criterion(outputs, labels)`: loss between the output and the ground truth label.\n",
    "                    \n",
    "- In training mode, backward pass `backward()`: collect a new set of gradients which we propagate back into each of the network's parameters using `optimizer.step()`.\n",
    "\n",
    "- We'll also keep track of the progress with some printouts. In order to create a nice training curve later on we also create two lists for saving training and testing losses. On the x-axis we want to display the number of training examples the network has seen during training.\n",
    "\n",
    "- Save model state: Neural network modules as well as optimizers have the ability to save and load their internal state using `.state_dict()`. With this we can continue training from previously saved state dicts if needed - we'd just need to call `.load_state_dict(state_dict)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load train_val_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load train_val_model.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "def train_val_model(model, criterion, optimizer, dataloaders, num_epochs=25,\n",
    "        scheduler=None, log_interval=None):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Store losses and accuracies accross epochs\n",
    "    losses, accuracies = dict(train=[], val=[]), dict(train=[], val=[])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if log_interval is not None and epoch % log_interval == 0:\n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            nsamples = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                nsamples += inputs.shape[0]\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if scheduler is not None and phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            #nsamples = dataloaders[phase].dataset.data.shape[0]\n",
    "            epoch_loss = running_loss / nsamples\n",
    "            epoch_acc = running_corrects.double() / nsamples\n",
    "\n",
    "            losses[phase].append(epoch_loss)\n",
    "            accuracies[phase].append(epoch_acc)\n",
    "            if log_interval is not None and epoch % log_interval == 0:\n",
    "                print('{} Loss: {:.4f} Acc: {:.2f}%'.format(\n",
    "                    phase, epoch_loss, 100 * epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        if log_interval is not None and epoch % log_interval == 0:\n",
    "            print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.2f}%'.format(100 * best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, losses, accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one epoch and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerMLP(D_in, 50, D_out).to(device)\n",
    "print(next(model.parameters()).is_cuda)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Explore the model\n",
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)\n",
    "\n",
    "print(\"Total number of parameters =\", np.sum([np.prod(parameter.shape) for parameter in model.parameters()]))\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer, dataloaders,\n",
    "                       num_epochs=1, log_interval=1)\n",
    "\n",
    "print(next(model.parameters()).is_cuda)\n",
    "torch.save(model.state_dict(), 'models/mod-%s.pth' % model.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the model to make new predictions. Consider the device, ie, load data on device `example_data.to(device)` from prediction, then move back to cpu `example_data.cpu()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx, (example_data, example_targets) = next(enumerate(val_loader))\n",
    "example_data = example_data.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  output = model(example_data).cpu()\n",
    "\n",
    "example_data = example_data.cpu()\n",
    "\n",
    "#Â print(output.is_cuda)\n",
    "\n",
    "# Softmax predictions \n",
    "preds = output.argmax(dim=1)\n",
    "\n",
    "print(\"Output shape=\", output.shape, \"label shape=\", preds.shape)\n",
    "print(\"Accuracy = {:.2f}%\".format((example_targets == preds).sum().item() * 100. / len(example_targets)))\n",
    "\n",
    "show_data_label_prediction(data=example_data, y_true=example_targets, y_pred=preds, shape=(3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot missclassified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = example_targets != preds\n",
    "#print(errors, np.where(errors))\n",
    "print(\"Nb errors = {}, (Error rate = {:.2f}%)\".format(errors.sum(), 100 * errors.sum().item() / len(errors)))\n",
    "err_idx = np.where(errors)[0]\n",
    "show_data_label_prediction(data=example_data[err_idx], y_true=example_targets[err_idx],\n",
    "                           y_pred=preds[err_idx], shape=(3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continue training from checkpoints: reload the model and run 10 more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerMLP(D_in, 50, D_out)\n",
    "model.load_state_dict(torch.load('models/mod-%s.pth' % model.__class__.__name__))\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer, dataloaders,\n",
    "                       num_epochs=10, log_interval=2)\n",
    "\n",
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test several MLP architectures\n",
    "\n",
    "- Define a `MultiLayerMLP([D_in, 512, 256, 128, 64, D_out])` class that take the size of the layers as parameters of the constructor.\n",
    "- Add some non-linearity with relu acivation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, d_layer):\n",
    "        super(MLP, self).__init__()\n",
    "        self.d_layer = d_layer\n",
    "        layer_list = [nn.Linear(d_layer[l], d_layer[l+1]) for l in range(len(d_layer) - 1)]\n",
    "        self.linears = nn.ModuleList(layer_list)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, self.d_layer[0])\n",
    "        # relu(Wl x) for all hidden layer\n",
    "        for layer in self.linears[:-1]:\n",
    "            X = F.relu(layer(X))\n",
    "        # softmax(Wl x) for output layer\n",
    "        return F.log_softmax(self.linears[-1](X), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP([D_in, 512, 256, 128, 64, D_out]).to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer, dataloaders,\n",
    "                       num_epochs=10, log_interval=2)\n",
    "\n",
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the size of training dataset\n",
    "\n",
    "Reduce the size of the training dataset by considering only `10` minibatche for size`16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = load_mnist(16, 1000)\n",
    "\n",
    "train_size = 10 * 16\n",
    "\n",
    "# Stratified sub-sampling\n",
    "targets = train_loader.dataset.targets.numpy()\n",
    "nclasses = len(set(targets))\n",
    "\n",
    "indices = np.concatenate([np.random.choice(np.where(targets == lab)[0], int(train_size / nclasses),replace=False) \n",
    "    for lab in set(targets)])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_loader.dataset, batch_size=16,\n",
    "    sampler=torch.utils.data.SubsetRandomSampler(indices))\n",
    "\n",
    "# Check train subsampling\n",
    "train_labels = np.concatenate([labels.numpy() for inputs, labels in train_loader])\n",
    "print(\"Train size=\", len(train_labels), \" Train label count=\", {lab:np.sum(train_labels == lab) for lab in set(train_labels)})\n",
    "print(\"Batch sizes=\", [inputs.size(0) for inputs, labels in train_loader])\n",
    "\n",
    "# Put together train and val\n",
    "dataloaders = dict(train=train_loader, val=val_loader)\n",
    "                   \n",
    "# Info about the dataset\n",
    "D_in = np.prod(dataloaders[\"train\"].dataset.data.shape[1:])\n",
    "D_out = len(dataloaders[\"train\"].dataset.targets.unique())\n",
    "print(\"Datasets shape\", {x: dataloaders[x].dataset.data.shape for x in ['train', 'val']})\n",
    "print(\"N input features\", D_in, \"N output\", D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP([D_in, 512, 256, 128, 64, D_out]).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer, dataloaders,\n",
    "                       num_epochs=100, log_interval=20)\n",
    "\n",
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use an opimizer with an adaptative learning rate: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP([D_in, 512, 256, 128, 64, D_out]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer, dataloaders,\n",
    "                       num_epochs=100, log_interval=20)\n",
    "\n",
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MLP on CIFAR-10 dataset\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
    "\n",
    "Here are the classes in the dataset, as well as 10 random images from each:\n",
    "- airplane \t\t\t\t\t\t\t\t\t\t\n",
    "- automobile \t\t\t\t\t\t\t\t\t\t\n",
    "- bird \t\t\t\t\t\t\t\t\t\t\n",
    "- cat \t\t\t\t\t\t\t\t\t\t\n",
    "- deer \t\t\t\t\t\t\t\t\t\t\n",
    "- dog \t\t\t\t\t\t\t\t\t\t\n",
    "- frog \t\t\t\t\t\t\t\t\t\t\n",
    "- horse \t\t\t\t\t\t\t\t\t\t\n",
    "- ship \t\t\t\t\t\t\t\t\t\t\n",
    "- truck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "WD = os.path.join(Path.home(), \"data\", \"pystatml\", \"dl_cifar10_pytorch\")\n",
    "os.makedirs(WD, exist_ok=True)\n",
    "os.chdir(WD)\n",
    "print(\"Working dir is:\", os.getcwd())\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Image preprocessing modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
    "                                             train=True, \n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "\n",
    "val_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Put together train and val\n",
    "dataloaders = dict(train=train_loader, val=val_loader)\n",
    "\n",
    "# Info about the dataset\n",
    "D_in = np.prod(dataloaders[\"train\"].dataset.data.shape[1:])\n",
    "D_out = len(set(dataloaders[\"train\"].dataset.targets))\n",
    "print(\"Datasets shape:\", {x: dataloaders[x].dataset.data.shape for x in ['train', 'val']})\n",
    "print(\"N input features:\", D_in, \"N output:\", D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP([D_in, 512, 256, 128, 64, D_out]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer, dataloaders,\n",
    "                       num_epochs=50, log_interval=10)\n",
    "\n",
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
